---
title: Transfer of Learned Opponent Models in Zero Sum Games
subtitle: 
titlerunning: 
authorrunning: 
thanks: The authors have no conflicts of interest to declare that are relevant to the content of this article. This work was supported by the UK Engineering and Physical Sciences Research Council under grant EP/S515255/1. 

authors: 
- name: Ismail Guennouni
  address: Department of Experimental Psychology, University College London, 26 Bedford Way, London WC1H 0AP, United Kingdom
  email: i.guennouni.17@ucl.ac.uk
  
- name: Maarten Speekenbrink
  address: Department of Experimental Psychology, University College London, 26 Bedford Way, London WC1H 0AP, United Kingdom
  email: m.speekenbrink@ucl.ac.uk

keywords:
- Learning Transfer
- Opponent Modelling
- Hidden Markov Models
- Bayesian Cognitive Hierarchy

#PACS: 
#- PAC1
#- superPAC
    
# MSC:
# - MSC code 1
# - MSC code 2

abstract: |
  Human learning transfer abilities take advantage of important cognitive building blocks such as an abstract representation of concepts underlying tasks and causal models of the environment. One way to build abstract representations of the environment when the task involves interactions with others is to build a model of the opponent that may inform what actions they are likely to take next. In this study, we explore opponent modelling and its transfer in games where human agents play against computer agents with human-like limited degrees of iterated reasoning. In two experiments, we find that participants deviate from Nash equilibrium play and learn to adapt to their opponent's strategy to exploit it. Moreover, we show that participants transfer their learning to new games. Computational modelling shows that players start each game with a model-based learning strategy that facilitates between-game transfer of their opponent's strategy, but then switch to behaviour that is consistent with a model-free learning strategy in the latter stages of the interaction.

bibliography: Mendeley2.bib
biblio-style: spphys
# bibstyle options spbasic(default), spphys, spmpsci
output: rticles::springer_article
---
```{r setup, include=FALSE}
# knitr::opts_knit$set(root.dir = '../Report')
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(out.width = "\\textwidth")
```

```{r load-packages, include = FALSE}
library(papaja)
library(kableExtra)
require(knitr)
require(citr)
require(bookdown)

# using some functions dplyr, ggpubr, PairedData and sjPlot. Need to be loaded. 
#library(plyr)
library(tidyr)
library(dplyr)
library(MASS)
library(ggpubr)
library(afex)
library(PairedData)
library(multcompView)
library(lsmeans)
library(magick)
library(depmixS4)
library(flextable)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)

```


```{r, include=FALSE} 
options(tinytex.verbose = TRUE)
#add_wordcount_filter(NULL, error = FALSE)
```


# Introduction

Being able to transfer previously acquired knowledge to a new domain is one of the hallmarks of human intelligence. This ability relies on important cognitive building blocks, such as an abstract representation of concepts underlying tasks [@Lake2017]. One way to form these representations when the task involves interactions with others, is to build a model of the person we are interacting with that offers predictions of the actions they are likely to take next. There is evidence that people learn such models of their opponents when playing repeated economic games [@stahl1995players]. A model of the opponent can help increase performance in a particular game, but learning more general characteristics of an opponent may also help increase performance in other games. In this paper, we are specifically interested in the latter: How do people build and use models of their opponent to facilitate learning transfer? 

Repeated games, in which players interact repeatedly with the same opponent and have the ability to learn about their opponent's strategies and preferences [@mertens1990repeated] are particularly useful to address this question. The early literature on learning transfer in repeated games has mostly focused on the proportion of people who play normatively optimal (e.g. Nash equilibrium play) or use salient (e.g. risk dominance) actions in later games, having had experience with a similar game environment previously [@ho1998iterated; @knez2000]. As is well-known, a Nash equilibrium means that all players in a game act such that no-one can unilaterally improve their performance by deviating from their strategy. When playing against an opponent with a Nash-optimal strategy, you can do no better than play according to the Nash-equilibrium strategy as well. However, when faced with a player who deviates from the Nash-optimal strategy, you may be able to exploit this and also deviate from this strategy, increasing your performance beyond what is expected at Nash equilibrium. Of course, this comes with a risk, as your own deviation from Nash-optimal play may leave you open to similar exploitation. 

Studies that focused on whether people can learn to exploit deviations from Nash-equilibrium play have mostly looked at the ability of players to detect and exploit contingencies in their opponent's actions [@dyson2016negative; @spiliopoulos2013strategic; @shachat2004we]. These studies used computer opponents that don't adapt their strategy to their human opponent, mostly consisting of playing each action with a fixed probability (a mixed strategy) or using a pre-determined sequence. Findings show that humans are capable of adapting to non-Nash-equilibrium play and detect patterns in an opponent's history of play. However, the use of mixed strategies may have limited people's ability to form accurate opponent models.

<!-- Introduce cycle based and outcome based strategies -->
The game of Rock-Paper-Scissors (RPS) has emerged as a central paradigm to test sequential adversarial reasoning in repeated interactions. Beyond action frequencies, @dyson2019behavioural identifies cycle-based and outcome-based dependencies as important strategies in repeated RPS games. A positive cycle is when a player chooses in trial $t+1$ an action that would have beaten the action in trial $t$ (e.g., playing Paper after Rock), while a negative cycle goes in the opposite direction by choosing an action in trial $t+1$ that would have been beaten by the action on trial $t$ (e.g., playing Scissors after Rock). These cycle-based strategies can be based on the opponent's or a player's own past choices. For example, @eyler2009winning show that players have a tendency to repeat their previous-round actions. <!--, or choose actions that beat previous-round actions.--> @brockbank2021formalizing defined a hierarchy of strategies based on previous actions (base rate of choices) as well as cycle transitions (base rate of positive and negative cycles). Using data on dyads playing 300 rounds of RPS against the same opponent, they show that exploitation of non-random play leverages simple regularities between a player's next action and their own previous action, and between a player's next action and their opponent's previous action, while more complex regularities remain unexploited. 
Outcome based dependencies are another common basis for RPS strategies. The idea is to change the likelihood of future actions based on the outcome (win, loss, or tie) of the previous round. Heuristics such as win-stay/lose-shift are an example of such outcome dependencies. Players can also combine outcome and cycle based strategies, choosing to cycle in a positive or negative direction depending on whether the previous round was won or lost [@wang2014social; @xu2013cycle].

<!-- Can be framed in terms of level-k -->

Another way to frame these strategies is in terms of iterative reasoning. People may think of their opponents as applying a limited number of recursive steps to determine their next action. The type of reasoning we refer to takes the form of "I believe that you believe that I believe ...". For example, if I believe that you believe I will play Rock (perhaps because I played Rock in the previous round), then I would expect you to play Paper, as this beats my Rock. I would therefore choose to play Scissors, as this beats your Paper. This results in a self-directed negative cycle strategy (playing the action that beats ones previous action), but it is the result of applying one level of recursive reasoning (player A believes player B believes A will repeat their last action, and A best responds to player B's best response to this belief). <!--their opponent believes a repition their of will repeat Playing an action that would have beaten the opponent's previous action is an other-directed positive cycle strategy, and also consistent with a player using one level of recursive reasoning by assuming the opponent will repeat their last action and best responding to that.-->

<!-- But if I believe you apply similar reasoning, I might expect you to believe I will play Scissors, and therefore I would play Rock. But then I might expect you to play Paper in return, and so forth, ad infinitum.-->

Strategies based on iterative reasoning can explain non-equilibrium play in a range of games [@camerer2003behavioural], such as the $p$-beauty contest game [@nagel1995unraveling], dominance solvable games [@camerer2004cognitive], and various standard normal form games [@costa2001cognition]. They also underlies successful models in behavioural economics, such as Level-$k$ and Cognitive Hierarchy models [@camerer2004cognitive], which posit that people assume their opponent applies a limited level of iterative reasoning, taking actions that are best responses to those of their (modelled) opponent. In Level-$k$ theory, a level-0 player uses a fixed strategy without explicitly considering the strategy of their opponent. A level-1 player assumes their opponent is a level-0 player, and chooses actions to best respond to the strategy of their opponent, without considering what their opponent might believe that they will play. A level-2 player, on the other hand, takes their opponent's belief about their actions into account, assuming they face a level-1 player, and choosing actions to best respond to the actions of that player. Cognitive Hierarchy theory is based on similar principles, but rather than assuming an opponent always adopts a particular level-$k$ strategy, they are assumed to adopt each of the level-$k$ strategies with a particular probability (i.e., the opponent's strategy is a mixture over pure level-$k$ strategies). 

<!-- Better framing of our contribution 
While there are similarities between outcome and cycle based strategies and an opponent adopting a level-$k$ iterative reasoning, the aforementioned studies did not specifically frame the player's strategies in terms of iterative reasoning.--> @batzilis2019behavior analysed data from a million online RPS games and found that players strategically used information on their opponents’ previous play. While the majority of play (74%) is consistent with a level-0 strategy, a significant proportion is consistent with either a level-1 (19%) or level-2 strategy (7%). As noted above, iterative reasoning results in specific action contingencies which correspond to particular action, outcome, and cycle based strategies. Prior research analysing behaviour in terms of these latter strategies has not explicitly related these to levels of iterative reasoning. That particular patterns are observed more frequently, and are exploited more readily [@brockbank2021formalizing], may be due to people adopting specific forms of iterative reasoning. For example, using a sequential move game, @hedden_what_2002 found that people initially adopt a level-1 strategy, thus assuming their opponent to adopt a level-0 strategy, but can learn over time to adopt a level-2 strategy if their opponent consistently uses a level-1 strategy. In a different version of this sequential move game, @goodie_levels_2012 found that participants initially adopt a level-2 strategy, but over time can learn to adopt a level-1 strategy if the opponent consistently uses a level-0 strategy. To what extent these results generalise to simultaneous move games such as RPS is unclear. @zhang_rock-paper-scissors_2021 found that people can learn to beat computer opponents adopting a win-stay lose-change or win-change lose-stay strategy, which are both level-0 strategies. 

<!-- To what extent people exploit an opponent who consistently uses a level-k strategy remains unclear. -->

<!-- Still, whether computer agent following an explicit level-k strategy is exploitable  by humans and whether successfully exploiting such a player in an RPS game can generalise to structurally similar and dissimilar games is still an open question.** -->

There are three main ways in which you can learn to exploit a level-$k$ opponent. One way is to explicitly learn the depth of their iterative reasoning, such as that the opponent's actions are based on a level-2 strategy. The second way is to learn the contingencies between previous round play and an opponent's next action (e.g., that your opponent is likely to play Scissors if your previous action was Paper). Rather than learning to predict an opponent's next actions, and then deciding upon the best counter-move, a third strategy is to directly learn rewarding actions. Unlike learning contingencies between actions, or which current action is most rewarding, learning the depth of iterative reasoning allows for generalisation to other games with different actions. This can then provide an early advantage, allowing one to exploit the opponent's strategy before you have played long enough to reliably establish contingencies within a game. In what we call the Bayesian Cognitive Hierarchy model, we propose that people use Bayesian inference to determine the depth of iterative reasoning of their opponent, and use this to predict their opponent's actions and determine the best response to this. We contrast this to a Reinforcement Learning model, which learns which actions are most rewarding given previous round play, by learning state-action values (the expected reward from taking an action in a given state). The Experience Weighted Attraction (EWA) model [@ho2004economics], popular in behavioral game theory, includes an additional mechanism to learn about the consequences of actions that were not actually taken. 

As both RL and EWA models learn values for actions in states (here, we define the state as the previous play), they don't allow for generalization to new games with different actions, as there is no immediate way in which to map a set of actions in one game to a set of actions in another. For example, consider the game of Fire-Water-Grass (FWG), where water beats fire, grass beats water, and fire beats grass. This game is structurally the same as RPS. However, if you have learned in RPS to play Rock after a previous play of Paper, there is no direct way to transfer this to knowing that Fire is a rewarding move after previously having played Water. Learning about the new actions in FWG requires experience with the FWG game. This is in contrast to learning the level of iterative reasoning of the opponent. If you know they expect you to repeat your previous action and choose their action to beat this, then you can infer that they will play Grass after your previous choice of Water, even if you have never played FWG before.
<!-- Instead of learning that your opponent is likely to play Scissors if your previous action was Paper, and therefore responding with Rock, you could learn that it pays off to play Rock whenever you played Paper and your opponent played Scissors on the previous round. Such learning is implemented in model-free reinforcement learning models, which learn state-action values  -->
Whilst providing a way to transfer knowledge about an opponent to new games, iterative reasoning is likely to be more cognitively demanding than learning action contingencies. Once sufficient experience has been gained within a game, iterative reasoning may no longer provide an advantage over simpler reinforcement learning. Therefore, it might be more efficient to use an RL strategy in the later stages of a game. In our modelling, we will allow for such switching between strategies during games. 

In the present study, we let humans repeatedly face computer agents endowed with a limited ability for iterative reasoning based on previous-round play. As in previous studies discussed above, we use computer opponents to enable precise experimental control over the strategies used and the transfer of depth of iterative reasoning between games. We aim to assess whether (1) human players adapt their strategy to exploit this limited reasoning of their opponent, and (2) whether they are able to generalize a learned opponent model to other games. In two experiments, participants repeatedly face the same opponent (Experiment 1) or two different opponents (Experiment 2) in three consecutive games: the well-known Rock-Paper-Scissors game, the structurally similar Fire-Water-Grass game, and a less similar Numbers (Experiment 1) or Shootout (Experiment 2) game. 
To foreshadow our results, we find evidence that participants transfer the learned strategy of their opponent to new games, providing them with an early advantage before gaining enough experience within a game to enable learning contingencies between previous and winning actions. <!--This transfer is moderated by the sophistication of their opponent and the similarity between the games.--> Computational modelling shows that participants, when first encountering an opponent in a new game, employ Bayesian inference about the level of iterative reasoning of their opponent to predict their actions and determine the best response. However, in later rounds of the games, they switch to a cognitively less demanding reinforcement learning strategy.

<!--
# Formalizing opponent-modelling and learning transfer

**An opponent model encodes aspects of an opponent's strategy that allows one to predict their actions, so that one's own action can best respond to these. Take Rock-Paper-Scissors as an example. If you can predict that your opponent will next play Paper, you can counter this successfully by playing Scissors.**

**Predicting an opponent's next action can be based on learning contingencies within a game (e.g., that your opponent is likely to play Scissors if your previous action was Paper), but also on more abstract aspects, such as that the opponent's action are based on a level-2 strategy. Unlike contingencies between actions, such abstract aspects can be generalized to other games, with other actions. This can then provide an early advantage, allowing one to exploit the opponent's strategy before you have played long enough to reliably establish contingencies within a game. In what we call the Bayesian Cognitive Hierarchy model, we propose that people use Bayesian inference to determine the level of iterative reasoning of their opponent, and use this to predict their opponent's actions and determine the best response to this.**

**Rather than learning to predict an opponent's next actions, and then deciding upon the best counter-move, another plausible strategy is to directly learn rewarding actions. Instead of learning that your opponent is likely to play Scissors if your previous action was Paper, and therefore responding with Rock, you could learn that it pays off to play Rock whenever you played Paper on the previous round. Such learning is implemented in model-free reinforcement learning models, which learn state-action values (the expected reward from taking an action in a given state). The Experience Weighted Attraction (EWA) model [@ho2004economics], popular in behavioral game theory, includes an additional mechanism to learn about the consequences of actions that were not actually taken. As both RL and EWA models learn values for actions in states, they don't allow for generalization to new games with different actions, as there no immediate way in which to map a set of actions in one game to a set of actions in another. For example, consider the game of Fire-Water-Grass (FWG), where water beats fire, grass beats water, and fire beats grass. This game is structurally the same as RPS. However, if you have learned in RPS that playing Rock after a previous play of Paper, there is no direct way to transfer this to knowing that Fire is a rewarding move after previously having played Water. Learning about the new FWG actions requires experience with the FWG game. This is in contrast to learning the level of iterative reasoning of the opponent. If you know they expect you to repeat your previous action and choose their action to beat this, then you can infer that they will play Grass after your previous choice of Water, even if you haven't played FWG before.**

**In order to have a better understanding of the learning shown by participants, we fit various computational models to the data. We will distinguish between two main types of learning models. We first consider approaches that don't explicitly model the strategy of the opponent such as Reinforcement Learning (RL) models. To allow such models to learn the best action against a level-k player whose strategy depends on past trial actions, we use an RL model with a state space consisting of these actions.  An opponent with such a level-k strategy will ultimately be exploitable by participants using this type of learning model. Crucially however, such a model should not be able to account for zero shot generalisation to a new but isomorphic game since there is no model feature allowing immediate mapping between actions and states of these tasks.**

**The same can be said for other successful models from the learning literature in behavioral game theory such as the Experience Weighted Attraction (EWA) model [@ho2004economics] which combines two seemingly different approaches, namely reinforcement learning and belief learning. Belief learning models are based on the assumption that players keep track of their opponent's frequency of past actions, and best respond to that. By contrast, reinforcement learning does not explicitly take into account beliefs about other players, but simply increases the probability of repeating a more rewarding action. While the EWA model has been shown to perform better than either RL or belief learning alone in various repeated games, it is also incapable of zero shot generalisation of the strategy of an opponent using level-k iterated reasoning.**

**To account for the learning transfer, we need to specify models of decision making capable of abstracting away from the game features, and learning something about the opponent that is transferable. If we make participants play against an opponent that uses level-k iterated reasoning strategies, then we need a model capable of inferring the depth of reasoning of the opponent. To that end, we will use a Bayesian Cognitive Hierarchy (BCH) model in which the participant attempts to learn the type of opponent they are facing (its depth of reasoning) through Bayesian learning. Unlike the models above, the BCH model allows for between-game transfer, as knowledge of the level of the opponent can be used to immediately generate predictions in games that have not been played before, as long as there is data on a previous trial. This zero shot generalization is done simply by using the posterior $P(\text{level} = k|\mathcal{D}_T)$ from the previous game as the prior distribution in the next game.**

**Finally, we explore the possibility, in our computational treatment of experimental data, that participants are able to use different models at different stages of the game. It might be more efficient to use opponent model free learning in the latter stages of repeated interactions against the same opponent instead of keeping a more accurate but also more complex explicit opponent model. The latter however can be useful for early shot generalisation to new games but will be more cognitively taxing as the game progresses and more data is generated to allow reinforcing previously successful state dependent actions.**

**In the next section, we will describe in detail both experiments before formally defining the computational models discussed in this section.**

-->

# Experiment 1
In the first experiment, we aim to test learning transfer by making participants face the same computer opponent with a limited level of iterative reasoning in three sequential games that vary in similarity. If participants are able to learn the limitations of their opponent's iterative reasoning and generalize this to new games, their performance in (the early stages of) later games should be higher than expected if they were to completely learn a new strategy in each game. 

## Methods


### Participants and Design
A total of 52 (28 female, 24 male) participants were recruited on the Prolific Academic platform. The mean age of participants was 31.2 years. Participants were paid a fixed fee of £2.5 plus a bonus dependent on their performance (£1.06 on average). The experiment had a 2 (computer opponent: level-1 or level-2) by 3 (games: rock-paper-scissors, fire-water-grass, numbers) design, with repeated measures on the second factor. Participants were randomly assigned to one of the two levels of the first factor. 

### Tasks
Participants played three games against their computer opponent: Rock-Paper-Scissors (RPS), Fire-Water-Grass (FWG), and the Numbers game. RPS is a 3x3 zero-sum game, with a cyclical hierarchy between the two player's actions: Rock blunts Scissors, Paper wraps Rock, and Scissors cut Paper. If one player chooses an action which dominates their opponent's action, the player wins (receives a reward of 1) and the other player loses (receives a reward of -1). Otherwise it is a draw and both players receive a reward of 0. RPS has a unique mixed-strategy Nash equilibrium, which consists of each player in each round randomly selecting from the three options with uniform probability. 

The FWG game is identical to RPS in all but action labels: Fire burns Grass, Water extinguishes Fire, and Grass absorbs Water. We use this game as we are interested in whether learning is transferred to a fundamentally similar game, where the only difference is in the label of the possible actions. This should make it relatively easy to generalize knowledge of the opponent's strategy, provided this knowledge is on a sufficiently abstract level, such as knowing the opponent is a level-1 or level-2 player. Crucially, learning simple contingencies such as "If I played Rock on the previous round, playing Scissors next will likely result in a win", is not generalizable to this similar game, as these contingencies are tied to the labels of the actions. 

The Numbers game is a generalization of RPS. In the variant we use, 2 participants concurrently pick a number between 1 and 5. To win in this game, a participant needs to pick a number exactly 1 higher than the number chosen by their opponent. For example, if a participant thinks their opponent will pick 3, they ought to choose 4 to win the round. To make the strategies cyclical as in RPS, the game stipulates that the lowest number (1) beats the highest number (5), so if the participant thinks the opponent will play 5, then the winning choice is to pick 1. This game has a structure similar to RPS in which every action is dominated by exactly one other action. All other possible combinations of choices are considered ties. Similar to RPS and FWG, the mixed-strategy Nash equilibrium is to randomly choose each action with equal probability.

The computer opponent was programmed to use either a level-1 or level-2 strategy in all the games. A level-1 player is defined as a player who best responds to a level-0 player. A level-0 player does not consider their opponent's beliefs. Here, we assume a level-0 player simply repeats their previous action. There are other ways to define a level-0 player. For instance, as repeating their action if it resulted in a win and choosing randomly from the remaining actions otherwise, or choosing randomly from all actions. As a best response to a uniformly random action is itself a random action, defining a level-0 player in such a way would make a level-1 opponent's strategy much harder to discern. Because we are mainly interested in generalization of knowledge of an opponent's strategy to other games, which requires good knowledge of this strategy, we opted for this more deterministic formulation of a level-0 player. Repeating previous actions is also in line with findings of @eyler2009winning. So the level-1 computer agent expects their (human) opponent to repeat their previous action, and chooses the action that would beat this. The level-2 computer opponent assumes in turn that the participant is a level-1 opponent, playing according to the strategy just described. To make the computer opponent's strategy not too obvious, we introduced some randomness in their actions, making them play randomly in 10\% of all trials. Note that at all levels, the strategies are contingent on the actions taken in the previous round. The choice of this type of strategy is consistent with evidence that humans strategically use information from last round play of their opponents in zero-sum games [@batzilis2019behavior; @wang2014social]. Table \ref{tab:table-actions} shows an example of the computer opponent's actions in response to the previous round play.

```{r table-actions, warning = FALSE, ft.align="center", tab.id='table-actions', label='table-actions'}
library(flextable)
df <- data.frame( c("Paper","Scissors","Rock","..."),
                  c("Rock","Scissors","Paper","..."),
                  c("Scissors","Rock","Paper","..."),
                  c("Scissors","Paper","Rock","..."))
colnames(df) <- c("Human action t-1","Computer action t-1","Computer level-1 action t", "Computer level-2 action t")

knitr::kable(df,booktabs = TRUE,
  caption = 'Example of how a level-1 and level-2 computer agent plays in response to actions taken in the previous round', ) %>% kable_styling(latex_options="scale_down")
# df <- flextable(df)
# df <- set_caption(df, "Example of how a level-1 and level-2 computer agent plays in response to actions taken in the previous round")
#   autonum = run_autonum(seq_id = "tab", bkm = "actions"))

# df <- autofit(df)
# df
```


<!-- \begin{table}[bth!] -->
<!-- \caption{Example of how a level-1 and level-2 computer agent plays in response to actions taken in the previous round.} -->
<!-- \centering -->
<!-- \label{table:1} -->
<!-- \begin{tabular}{||c c | c c||} -->
<!--  \hline -->
<!--  Human action $t-1$ & Computer action $t-1$  & Computer level-1 action $t$ & Computer level-2 action $t$ \\ [0.5ex] -->
<!--  \hline\hline -->
<!--  Paper & Rock & Scissors & Scissors \\ -->
<!--  Scissors  & Scissors & Rock & Paper \\ -->
<!--  Rock & Paper & Paper & Rock \\ -->
<!--  ... & ... & ... & ... \\ [1ex] -->
<!--  \hline -->
<!-- \end{tabular} -->
<!-- \end{table} -->

### Procedure

Participants were informed they would play three different games against the same computer opponent. Participants were told that the opponent cannot cheat and will choose its actions simultaneously with them, without prior knowledge of the participant's choice. After providing informed consent and reading the instructions, participants answered a number of comprehension questions. They then played the three games against their opponent in the order RPS, FGW, and Numbers. An example of the interface for the RPS game is provided in Figure \ref{fig:feedback-rps-exp2}. On each round, the human player chooses an action, and after a random delay (between 0.5 and 3 seconds) is shown the action chosen by the computer opponent and the outcome of that round. A total of 50 rounds of each game was played with the player's score displayed at the end of each game. The score was calculated as the number of wins minus the number of losses. Ties did not affect the score. In order to incentivise the participants to maximise the number of wins against their opponent, players were paid a bonus at the end of the experiment proportional to their final score (each point was worth £0.02). After playing all the games, participants were asked questions about their beliefs about the computer opponent, related to whether they thought they learned their opponent's strategy, and how difficult they found playing against their opponent. They were then debriefed and thanked for their participation.

```{r feedback-rps-exp2,fig.cap = "Screenshot of the Rock-Paper-Scissors game in Experiment 2. Shown here is the feedback stage, after both the human (left) and computer (right) players have chosen their action. The interface was similar in Experiment 1, but excluded the history of game play in the center panel.", fig.align='center'}

knitr::include_graphics("../images/feedback_rps.png")

```

```{r load-exp1-data, include=FALSE}

dat1 <- read.csv("../data20180719.csv") 

# transform 'winner' variable in numeric score
dat1$score <- recode(dat1$winner, human = 1, tie = 0, ai = -1)
# create a new variable 'block' with round 1...25 = block 1 and round 26...50 as block 2
dat1$block <- factor(as.numeric(cut(dat1$round,2)),labels =c("first half", "second half"))

# create a new variable "game.f" as a factor variable of games
dat1$game.f <- factor(dat1$game, labels = c("RPS","FWG","Numbers"),levels=c("rps","fwg","numbers"))

#Group data by human_id and calculate mean score per block of each game.
dat2 <- dat1 %>% 
  group_by(human_id,condition,game,block,game.f) %>% 
      summarize(block_score = mean(score))

# Group data by game and ID
dat3 <- group_by(dat2, human_id,game.f,game) %>% summarise(game_score = mean(block_score))
# head(dat3,6)

# Subsetting scores data by game
rps <- subset(dat3,  game.f == "RPS",game_score)
fwg <- subset(dat3,  game.f == "FWG",game_score)
num <- subset(dat3,  game.f == "Numbers",game_score)

```

```{r exp1-avg-scores-game, fig.cap = "Performance per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align="center", fig.width=8, fig.height = 4}
# Plot average  scores per game

dat2 %>%
  mutate(game = factor(game)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Numbers"))) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggplot(aes(x = game, y = block_score, colour = block)) + stat_summary(fun.data = "mean_ci", position = position_dodge2(width=.8), geom = "crossbar")  + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Score") + xlab("Game") + facet_grid(. ~ condition) + theme_apa() + geom_hline(yintercept = 0, lty=2)


```

## Behavioral results

Participants' scores in each half of each game are depicted in Figure \ref{fig:exp1-avg-scores-game}. <!-- On average, participants obtained the lowest score in the RPS game ($M = 0.289$, $SD = 0.348$), the highest in the FWG game ($M = 0.454$, $SD = 0.354$), with an intermediate score in Numbers ($M = 0.31$, $SD = 0.347$). <!-- Participants' performance was highest in the FWG game ($M = 0.454$, $SD = 0.354$).--> Overall, scores in each game were significantly higher than 0, the expected score of uniformly random play (RPS: `r papaja::apa_print(t.test(subset(dat3, game.f == "RPS")$game_score))$full_result`; FWG:  `r papaja::apa_print(t.test(subset(dat3, game.f == "FWG")$game_score))$full_result`; Numbers: `r papaja::apa_print(t.test(subset(dat3, game.f == "Numbers")$game_score))$full_result`). <!-- $t(51) = 7.26$, $p < .001$;  FWG:  $t(51) = 10.04$, $p < .001$; Numbers: $t(51) = 7.17$, $p < .001$).--> As uniformly random play is the Nash equilibrium, this indicates successful deviation from a Nash-optimal strategy. <!-- A linear mixed-effects model, with fixed effects for Game, Opponent, and Block, as well as all interactions, and participant-wise random intercepts, showed a main effect of Opponent, $F(1,50) = 5.44$, $\eta^{2} = 0.10$, $p = .024$, which is due to participants scoring higher against the level-1 ($M = 0.43$) than against the level-2 opponent ($M = 0.27$). Note that the strategies of both type of opponent are equally consistent, and hence in principle equally exploitable. The difference in performance against the two types of player indicates that participants found it more difficult to predict the next actions of the more sophisticated level-2 opponent than the comparatively less sophisticated level-1 opponent. This is consistent with participants employing iterative reasoning themselves, rather than simple contingency learning, as such reasoning is more involved for a level-2 than a level-1 opponent. A significant main effect of Block, $F(2,250) = 15.16$ , $\eta^{2} = 0.06$, $p < .001$, shows that the score in the first half of each game ($M = 0.29$) was significantly lower than in the second half ($M = 0.40$), reflecting within-game learning. A main effect of Game, $F(2,250) = 12.90$, $\eta^{2}  = 0.09$,  $p < .001$, reflects that participants obtained the lowest score in the RPS game ($M = 0.289$), the highest in the FWG game ($M = 0.454$), with an intermediate score in Numbers ($M = 0.31$).--> Additional analysis (see Supplementary Information) indicates better performance against the level-1 compared to level-2 opponent. This indicates that participants may have found it more difficult to predict the actions of the more sophisticated level-2 opponent, even though both types of opponent are equally consistent and hence equally exploitable in principle.

```{r MIXEDexp1, include = FALSE, warning=FALSE}

# tmp <- afex::aov_car(
#   block_score ~ game.f*condition*block + Error(human_id/(game.f*block))
#   , data=dat2
#   , type = 3)

#apa_lm1 <- apa_print(exp1_early_anova)

#means.int2 <- lsmeans(exp1_early_anova, specs = c("game.f","condition"))
#trans26 <- summary(means.int2, infer = c(TRUE,TRUE),level = .95, adjust = "holm",ref=c("FWG","Numbers"))
                                    
mod1 <- mixed(block_score ~ game.f*condition*block + (1 | human_id),data=dat2,check_contrasts = FALSE, type = 3, method = "KR")
#summary(mod1)
mod1

mod.lsm <- lsmeans(mod1, ~ game.f*condition, adjust="holm")

#contrast(mod.lsm, "consec",simple = "each",combine = TRUE, adjust = "tukey")
#contrast(mod.lsm, interaction = c("game.f", "condition"))




# Get effect sizes from F values of mixed effect model
library(effectsize)
F_to_eta2(12.9, 2, 250)
F_to_eta2(5.44, 1, 50)
F_to_eta2(15.16, 1, 250)
F_to_eta2(4.52, 2, 250)

```
<!-- To assess learning within and between games, we estimate a linear mixed-effects model with score as the dependent variable, and Condition (Level 1 or Level 2), Game (RPS, FWG, Numbers) and Block (first-half , second-half) as independent variables, including all main effects and interactions. The model included random intercepts for participants. The results show a main effect of Game ($F(2,250) = 12.90$, $\eta^{2}  = 0.09$,  $p < .001$).  -->

<!-- Post-hoc pairwise comparisons show that performance in the FWG game was significantly higher than in the RPS game ($t(100) =3.78$, $p < .001$) and the Numbers game ($t(100) = 3.32$ , $p = .002$). The score in RPS was not significantly different from the score in Numbers ($t(100) = 0.45$ , $p = .65$). The main effect of Block ($F(2,250) = 15.16$ , $\eta^{2} = 0.06$, $p < .001$) shows that the score in the first half of each game ($M = 0.29$) was significantly lower than in the second half ($M = 0.40$), which indicates within-game learning.  -->

<!-- The main effect of Condition ($F(1,50) = 5.44$, $\eta^{2} = 0.10$, $p = .024$) indicates that scores were higher against the level-1 player ($M = 0.43$) than against the level-2 player ($M = 0.27$). Thus, it appears that is was harder for participants to exploit the strategy of the more sophisticated level-2 opponent than the comparatively less sophisticated level-1 opponent.  -->

<!-- There was also an interaction between Game and Condition ($F(2,250) = 4.52$ ,  $\eta^{2} = 0.03$, $p = .01$). Post-hoc pairwise comparisons ($p$-values adjusted using Holm method for multiple comparisons) show that, for participants facing level-1 opponents, scores on the FWG game were significantly higher than those for both RPS ($t(250) = 3.85$ , $p = .002$), and Numbers ($t(250) = 3.79$ , $p = .002$). For participants facing level-2 opponents however, there was no significant difference between scores on the three games. -->



```{r exp1-3factor-plot, include= FALSE, fig.cap = "Performance in early rounds (2-6) per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align = "center", fig.width=8, fig.height=4}

# Plot results by game and block (for all 3 games, learning is happening) 
dat2 %>%
  mutate(game = forcats::fct_recode(as.factor(game), RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggboxplot(x = "game", y = "block_score", palette = c("#00AFBB", "#E7B800"),order = c("RPS", "FWG","Numbers"), fill="block", ylab = "Percentage score", xlab = "Games") +  facet_grid(. ~ condition)
```

<!-- ## Learning transfer -->


```{r exp1-early-rounds, include=FALSE}
dat_26 <- subset(dat1,round >1 & round <7, drop =TRUE)
exp1_dat2_6 <- dat_26 %>% 
  group_by(human_id,condition,game.f,confidence,difficulty) %>% 
      summarise(early_score = mean(score))

# Check group means and SDs by game and condition
group_by(exp1_dat2_6, game.f,condition) %>%
  summarise(
    count = n(),
    mean = mean(early_score, na.rm = TRUE),
    sd = sd(early_score, na.rm = TRUE)
    )

early_rps1 <- subset(exp1_dat2_6,  game.f == "RPS",early_score)
t.test(early_rps1$early_score,mu=0, alternative = "two.sided")

early_fwg1 <- subset(exp1_dat2_6,  game.f == "FWG",early_score)
t.test(early_fwg1$early_score,mu=0, alternative = "two.sided")

early_num1 <- subset(exp1_dat2_6,  game.f == "Numbers",early_score)
t.test(early_num1$early_score,mu=0, alternative = "two.sided")
```

For an initial assessment of learning transfer, we focus on participants’ scores in the initial 5 rounds after the first round (rounds 2-6) of each game (see Figure \ref{fig:exp1-early-score-by-opp}). We exclude the first round as the computer opponent played randomly here and there is no opportunity yet for the human player to exploit their opponent's strategy. Players with no knowledge of their opponent's strategy would be expected to perform at chance level in these early rounds, whilst positive scores in rounds 2-6 are consistent with generalization of prior experience. The early round score in both FWG and Numbers is significantly higher than 0 (FWG: `r papaja::apa_print(t.test(early_fwg1$early_score,mu=0, alternative = "two.sided"))$full_result`; Numbers: `r papaja::apa_print(t.test(early_num1$early_score,mu=0, alternative = "two.sided"))$full_result`). <!--   $t(51) = 4.13$, $p < .001$; Numbers: $t(51) = 3.48$, $p = .001$).--> We did not expect positive early scores for the RPS game, as it was the first game played and there was no opportunity for learning about the opponent's strategy. Scores in this game were indeed not significantly different from 0 (`r papaja::apa_print(t.test(early_rps1$early_score,mu=0, alternative = "two.sided"))$full_result`). Additional analysis (see Supplementary Information) provided no evidence that learning transfer differed between the two types of opponent. <!-- ($t(51) = 1.08$, $p = .29$). -->


```{r, include=FALSE}
library(afex)

# setting contrast to compare early score between games across levels 
exp1_dat2_6$game.f <- as.factor(exp1_dat2_6$game.f)
exp1_dat2_6$condition <- as.factor(exp1_dat2_6$condition)

contrasts(exp1_dat2_6$game.f) <- cbind(c(1,-1,0),c(0,-1,1))
contrasts(exp1_dat2_6$condition) <- c(1,-1)

# Mixed effects Model: Random intercept for participants
lme_early <- mixed(early_score ~ game.f*condition + (1 | human_id),data=exp1_dat2_6,check_contrasts = FALSE, type = 3, method = "KR")
summary(lme_early)

lme_early

```


```{r exp1-early-score-by-opp, fig.cap = "\\label{ref:figure4-caption}Performance in early rounds (2-6) per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align ='center', fig.width=8, fig.height=4}

exp1_dat2_6 %>%
  mutate(game = factor(game.f)) %>%
  mutate(game = forcats::fct_recode(game, Numbers = "Numbers")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Numbers"))) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggplot(aes(x = game, y = early_score, colour=game)) + stat_summary(fun.data = "mean_ci", geom = "crossbar")  + geom_jitter(alpha=.4) + ylab("Early score") + xlab("Game") + facet_grid(. ~ condition) + theme_apa() + geom_hline(yintercept = 0, lty=2) + scale_color_discrete() + theme(legend.position = "none")

```

<!--
Next, we explored whether learning transfer is moderated by the type of opponent and game similarity. We expected better transfer between more similar games (i.e. better transfer from RPS to FWG than from RPS/FWG to Numbers), and worse transfer for the more sophisticated level-2 agent. Figure \ref{fig:exp1-early-score-by-opp} indicates that the pattern over the games is indeed dissimilar between level-1 and level-2 opponents. We estimated a linear mixed-effects model with the early round score as the dependent variable, and Opponent (Level 1 or Level 2) and Game (RPS, FWG, Numbers), as well as their interaction, as independent variables. In addition to these fixed effects, the model included random intercepts for participants. The results showed only a significant main effect of Game ($F(2,100) = 3.35$, $p = .04$). --><!--, but no effect for condition. Further investigation of the effects of game on early score using planned contrasts revealed that early scores in the FWG games are significantly higher than in the RPS ( $t(99.99) = -2.32$, $p = 0.02$).-->

<!--We next ran statistical tests on early round scores by game and opponent, against the null hypothesis of 0 (no transfer). For level-1 facing players, there is evidence of learning transfer from RPS to both FWG ($t(150) = 3.96$, $p < .001$) and Numbers ($t(150) = 3.74$, $p < .001$). For level-2 facing players, there is evidence for transfer from RPS to the similar game FWG, albeit scores are lower than for level-1 player ($t(150) = 2.48$, $p = .01$) but not to the dissimilar game of Numbers. 
-->

<!-- 
## Discussion

The results of Experiment 1 indicate that participants were able to learn successful strategies which exploited the deviation from Nash-optimal play of their opponents. Moreover, they were able to transfer knowledge about their opponent to other games, but this was moderated by the type of game. There was evidence of transfer from the RPS game to the similar FWG game for both opponents. However, transfer to the dissimilar Numbers game was only evident for participants facing a level-1 opponent. -->

```{r ANOVA early exp1, include = FALSE, warning=FALSE}

exp1_early_anova <- afex :: aov_car(
  early_score ~ game.f*condition + Error(human_id/(game.f))
  , data=exp1_dat2_6
  , type = 3)
                                    
exp1_early_anova
#apa_lm1 <- apa_print(exp1_early_anova)

means.int2 <- lsmeans(exp1_early_anova, specs = c("game.f","condition"))
trans26 <- summary(means.int2, infer = c(TRUE,TRUE),level = .95, adjust = "holm",ref=c("FWG","Numbers"))

```
  
```{r table-early-scores, include = FALSE, results = "asis"}
# apa_table(trans26, caption = "Average early round scores by game and condition in first experiment")
```


# Experiment 2

The results of Experiment 1 indicate that participants were able to learn successful strategies which exploited the deviation from Nash-optimal play of their opponents. Moreover, they were able to transfer knowledge about their opponent to later games. <!--, but this was moderated by the type of game. There was evidence of transfer from the RPS game to the similar FWG game for both opponents. However, transfer to the dissimilar Numbers game was only evident for participants facing a level-1 opponent. -->
In Experiment 2, we aimed to obtain a stronger test of learning transfer. Instead of facing a single level-1 or level-2 opponent throughout all games, participants now faced both types of opponent. To perform well against both opponents, participants would need to learn distinct strategies against these opponents. To reduce effects of increased memory load due to facing distinct opponents, we provided participants access to the history of play against an opponent within each game (see Figure \ref{fig:feedback-rps-exp2}). Finally, we changed the third game to a penalty Shootout game, with participants aiming to score a goal and opponents playing the role of goal keepers. Whilst this game has the same number of actions as the first two (aim left, center, or right), it is strategically dissimilar. Unlike the Numbers game in Experiment 1, the Shootout game does not have a cyclical hierarchy between actions, making it harder to win through a heuristic based on this cyclicity.

## Methods 
### Participants & Design 
A total of 50 participants (21 females, 28 males, 1 unknown) were recruited via the Prolific platform, none of which took part in Experiment 1. The average age was 30.2 years, and the mean duration to complete the task was 39 minutes. Participants received a fixed fee of £2.5 for completing the experiment and a performance dependent bonus (£1.32 on average).

### Tasks
Participants played three games: Rock-Paper-Scissors (RPS), Fire-Water-Grass (FWG), and the penalty Shootout game. The first two games were identical to the ones used in Experiment 1. In the Shootout game, participants took the role of the a football (soccer) player in a penalty situation, with the computer opponent taking the role of the goalkeeper. Players had the choice between three actions: shooting the football to the left, right, or centre of the goal. Similarly, the goalkeeper chooses between defending the left, right, or centre of the goal. If participants shoot in a different direction than where the goalkeeper defends, they win the round and the goalkeeper loses. Otherwise, the goalkeeper catches the ball and the player loses the round. There is no possibility of ties in this game. Figure \ref{fig:screenshot-shootout} shows a snapshot of play in the Shootout game. What makes this game different to the other games is that there are now two ways to beat the opponent: if the shooter thinks their opponent is going to choose to defend "right" in the next round, they can win by either choosing to shoot "left" or "center". A level-1 shooter who thinks that their goalkeeper opponent will repeat their last action has thus two possible best responses. A level-1 goalkeeper, however, has only a single best response (defending where their opponent aimed in the last round). A level-2 goalkeeper, who believes their opponent is a level-1 shooter, will have two best responses however. We programmed the level-2 computer player to choose randomly between these two best responses.

As in Experiment 1, all the games have a unique mixed-strategy Nash equilibrium consisting of uniformly random actions. If participants follow this strategy, or simply don't engage in learning how the opponent plays, they would score 0 on average against both level-1 and level-2 players. Evidence of sustained wins would indicate that participants have learned to exploit patterns in their opponents' play. 

```{r screenshot-shootout, fig.cap = "Screenshot of the shootout game", fig.align='center'}

knitr::include_graphics("../images/shootout.jpg")
```

### Procedure 
Participants played 3 games sequentially against both level-1 and level-2 computer opponents. As in Experiment 1, the computer opponents retained the same strategy throughout the 3 games. Participants faced each opponent twice in each game. Each game was divided into 4 stages, numbered 1 to 4, consisting of 20, 20, 10, and 10 rounds respectively for a total of 60 rounds per game. Participants started by facing one of the opponents in stage one, then the other in stage two. This was repeated in the same order in stages 3 and 4. Which opponent they faced first was counterbalanced. All participants engaged in the three games (RPS, FWG and Shootout) in this exact order, and were aware that their opponent could not cheat and chose their action simultaneously with the player, without knowing their choices beforehand. In order to encourage participants to think about their next choice, a countdown timer of 3 seconds was introduced at the beginning of each round. During those 3 seconds, participants could not choose an action and had to wait for the timer to run out. A random delay between 0.5 and 3 seconds was again introduced before the choice of the computer agent was revealed, as a way of simulating a real human opponent's decision time. After each round, participants were given detailed feedback about their opponent's action and whether they won or lost the round. Further information about the history of play in previous rounds was also provided and participants could scroll down to recall the full history of each interaction against an opponent in a particular stage of a game. The number of wins, losses and ties in each game were displayed at the top of the screen, and this scoreboard was reinitialised to zero at the onset of a new stage game.



```{r load_exp2_data , include=FALSE}
dat2 <- read.csv(file = "../Experiment_2/dat_exp2.csv")
dat_int <- read.csv(file = "../Experiment_2/dat_int_exp2.csv")
dat_int2 <- dat2 %>%
  mutate(game.f = factor(game, labels = c("RPS","FWG","Shoot"), levels=c("rps","fwg","shootout")),
         encounter = factor(interaction_lvl, labels=c("first","second"), levels=c("first interaction","second interaction")),
         condition = condition.f,
         opponent = opp_type) %>%
  group_by(human_id, game.f) %>% 
  summarise(game_score = mean(adj_score))

exp2_dat2_6 <- subset(dat2, (round >1 & round < 7) & (interaction_lvl == "first interaction"), drop =TRUE) %>%
  # mutate(game.f = factor(game.f, labels = c("RPS","FWG","Shoot"), levels=c("rps","fwg","shootout"))) %>%
  group_by(human_id, condition.f, game.f, opp_type, stage) %>%
  summarise(early_score = mean(adj_score))
```

## Behavioral results
Participants' scores are depicted in Figure \ref{fig:exp2-score-by-opp}.^[Scores in the Shootout game were adjusted because there were 2 out of three possible winning actions in that game, compared to one out of three in RPS and FWG. As such, the expected score of random play in the Shootout game is 1/3, whilst it is 0 in the RPS and FWG game. To make the scores comparable, we therefore subtracted 1/3 from the Shootout scores.] Average (adjusted) scores differed from 0 in all three games
(RPS: `r papaja::apa_print(t.test(subset(dat_int2, game.f == "RPS")$game_score))$full_result`; FWG:  `r papaja::apa_print(t.test(subset(dat_int2, game.f == "FWG")$game_score))$full_result`; Shootout:  `r papaja::apa_print(t.test(subset(dat_int2, game.f == "Shoot")$game_score))$full_result`).
<!-- (RPS: $t(49) = 6.26$, $p < .001$; FWG: $t(49) = 7.25$, $p < .001$; Shootout: $t(49) = 13.61$, $p < .001$).--> As in Experiment 1, this indicates that participants successfully deviated from Nash-optimal (random) strategies. In contrast to Experiment 1, additional analysis (see Supplementary Information) showed no overall difference in performance between the two types of opponent. However, participants performed better against the level-2 than level-1 opponent in RPG, but better against the level-1 opponent in Shootout. 

```{r exp2-score-by-opp, fig.cap = "\\label{fig:exp2-score-by-opp}Performance per game and interaction across opponents in Experiment 2. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align = "center", fig.width=8, fig.height=4}
# Plot results by game and block (for all 3 games, learning is happening) 
dat_int %>%
  mutate(game.f = forcats::fct_recode(game.f, Shootout = "SHOOT")) %>%
  mutate(game.f = factor(game.f, levels = c("RPS", "FWG", "Shootout"))) %>%
  mutate(opp_type = forcats::fct_recode(as.factor(opp_type), "Level 1" = "Level_1", "Level 2" = "Level_2")) %>%
  mutate(encounter = interaction_lvl) %>%
  ggplot(aes(x = game.f, y = int_score, colour = encounter)) + stat_summary(fun.data = "mean_ci", position = position_dodge2(width=.8), geom = "crossbar")  + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Score") + xlab("Game") + facet_grid(. ~ opp_type) + theme_apa() + geom_hline(yintercept = 0, lty=2)
```


<!-- To explore whether learning occurred within and between games, we estimated a linear mixed-effects model, with score as the dependent variable and Condition (level-1 first, level-2 first), Opponent type (level-1 or level-2), Game (RPS, FWG, Shootout) and Encounter (first or second) as the independent variables, including all main effects and interactions. The model included random intercepts for participants. We found a main effect of Game ($F(2,528) = 15.32$, $\eta^{2} = 0.04$,  $p < .001$). Post-hoc pairwise comparisons between games ($p$-values adjusted using Holm method for multiple comparisons) indicate performance in the games increased steadily throughout the experiment, with FWG performance significantly higher than RPS ($t(96) =2.53$, $p = .025$), and performance in the Shootout game significantly higher than in FWG ($t(96) = 2.32$, $p = .025$). Whilst the model shows no main effect of Condition, Opponent type, or Encounter, there was a significant interaction between Game and Opponent type ($F(2, 528) = 5.78$ ,$\eta^{2} = 0.02$,  $p = .003$). Follow-up analysis differentiating performance by type of opponent faced shows that when facing level-1 agents, scores increased steadily after each game, with FWG score significantly higher than RPS ($t(191) = 2.70$, $p = .03$) and Shootout scores in turn significantly higher than FWG  ($t(191) = 3.05$, $p = .01$). There was no significant difference between scores on games when facing level-2 agents however. -->

<!-- ## Learning transfer -->

As an initial assessment of transfer between games, we again focus on participants' scores in rounds 2-6 of each game, both for stage 1 and 2 when participants first interact with an opponent in a game (see Figure \ref{fig:exp2-early-score-by-opp}). We analyse these early-round scores separately for each stage, as early-round scores in the second stage could have benefited from experience in stage 1. Early-round scores differed significantly from 0 for the both the FWG (stage 1: `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 1 & game.f == "FWG")$early_score))$full_result`; stage 2: `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 2 & game.f == "FWG")$early_score))$full_result`) and Shootout (stage 1: `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 1 & game.f == "SHOOT")$early_score))$full_result`; stage 2: `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 2 & game.f == "SHOOT")$early_score))$full_result`). As expected, early scores in stage 1 of RPS did not differ significantly from 0, `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 1 & game.f == "RPS")$early_score))$full_result`. In stage 2, however, they did, `r papaja::apa_print(t.test(subset(exp2_dat2_6, stage == 2 & game.f == "RPS")$early_score))$full_result`. This indicates that the experience with the other opponent in stage 1 may have provided an advantage to playing against a different opponent in stage 2. Additional analysis (see Supplementary Information) indicates transfer to the Shootout game may have been easier for the level-1 than level-2 opponent. No reliable effect of stage was found. The latter is important, because if early-round scores are due to general practice effects rather then transfer of an opponent model, we would expect early-round scores to increase between stage 1 and 2 in a game.   

<!-- (level-1 opponent: $t(49) = 4.60$, $p < .001$, level-2 opponent: $t(49) = 4.20$, $p < .001$) and Shootout games (level-1 opponent: $t(49) = 6.80$, $p < .001$, level-2 opponent: $t(49) = 4.11$, $p < .001$)**. This is consistent with transfer of opponent models between games. **Early scores in the RPS game were, as expected, not significantly different from 0 for those facing level-1 opponents. However, for those facing level-2 opponents, the early scores were significantly different from 0 ($t(49) = 2.57$, $p = .01$). This was unexpected.**-->


<!-- We next estimated a linear mixed-effects model with the early round scores as the dependent variable, and Block (1, 2), Opponent type (level-1 or level-2) and Game (RPS, FWG, Shootout) as independent variables, including all main effects and interactions. In addition to these fixed effects, the model also included random intercepts for participants. We found a main effect of Game ($F(2,240) = 6.57$, $p = .002$), and an interaction between Game and Opponent type ($F(2,240) = 3.34$,  $p = .037$). The latter effect is due to early-score increasing from FWG to Shootout for level-1 opponents, but decreasing for level-2 opponents. We found no effect of Block. This is important, as if early-round scores are due to general practice effects, rather than transfer of an opponent model, we would expect early-round scores to increase between block 1 (first encounter of one opponent) and 2 (first encounter of other opponent) within a game. -->

<!-- No significant differences were found between participants facing different opponents in the same game, or between games when facing the same opponent.**The former is important as it is evidence against the above chance performance being driven mainly by practice effects. To be sure, if practice effects were driving the early round performance, we should see above chance scores when players are facing the second opponent in the same game, which is not the case here.** **I DON'T FOLLOW THIS. BUT THE ABSENCE OF AN INTERACTION BETWEEN CONDITION AND OPPONENT CAN BE USED FOR THIS ARGUMENT, I THINK. I SUGGEST THE FOLLOWING: We found no effect of Condition. This is important, as if early-round scores are due to general practice effects, rather than transfer of an opponent model, ** -->

<!--. The effect of game reflects early scores on FWG being significantly higher than on RPS ($t(240) = 3.01$, $p = .005$) as well as scores on Shootout being significantly higher than RPS ($t(240) = 3.25$, $p = .003$). We also found an interaction effect between Game and Opponent type ($F(2,240) = 3.34$,  $p = .037$). Post-hoc pairwise comparisons (p-values adjusted using Holm method for multiple comparisons) show that the difference between early scores in RPS and Shootout was mostly driven by participants facing level-1 players ($t(240) = 4.12$, $p < .001$). No significant differences were found between participants facing different opponents in the same game, or between games when facing the same opponent.**The former is important as it is evidence against the above chance performance being driven mainly by practice effects. To be sure, if practice effects were driving the early round performance, we should see above chance scores when players are facing the second opponent in the same game, which is not the case here.** **I DON'T FOLLOW THIS. BUT THE ABSENCE OF AN INTERACTION BETWEEN CONDITION AND OPPONENT CAN BE USED FOR THIS ARGUMENT, I THINK** -->

```{r exp2-early-score-by-opp, fig.cap = "\\label{ref:figure4-caption}Performance in early rounds (2-6) per game and opponent in Experiment 2. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align ='center', fig.width=8, fig.height=4}
exp2_dat2_6 %>%
  mutate(game = factor(game.f)) %>%
  mutate(game = forcats::fct_recode(game, Shootout = "SHOOT")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Shootout"))) %>%
  mutate(opp_type = factor(opp_type, levels = c("Level_1", "Level_2"))) %>%
  mutate(opp_type = forcats::fct_recode(opp_type, "Level 1" = "Level_1", "Level 2" = "Level_2")) %>%
  mutate(stage = factor(stage, levels = c(1,2))) %>%
  ggplot(aes(x = game, y = early_score, colour=stage)) + stat_summary(fun.data = "mean_ci", geom = "crossbar", position = position_dodge2(width=.8))  + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Early score") + xlab("Game") + facet_grid( ~ opp_type) + theme_apa() + geom_hline(yintercept = 0, lty=2) + scale_color_discrete()# + theme(legend.position = "none")
```

```{r, include = FALSE}
# testing differences between early scores across games 
library(afex)

exp2_dat2_6$game.f <- as.factor(exp2_dat2_6$game.f)
exp2_dat2_6$opp_type <- as.factor(exp2_dat2_6$opp_type)
exp2_dat2_6$condition.f <- as.factor(exp2_dat2_6$condition.f)
exp2_dat2_6$block <- 1
exp2_dat2_6$block[exp2_dat2_6$condition.f == "lvl_1 first" & exp2_dat2_6$opp_type == "Level_2"] <- 2
exp2_dat2_6$block[exp2_dat2_6$condition.f == "lvl_2 first" & exp2_dat2_6$opp_type == "Level_1"] <- 2
exp2_dat2_6$block <- factor(exp2_dat2_6$block)
contrasts(exp2_dat2_6$block) <- c(-1,1)
```


```{r, include=FALSE}
# Testing RPS early scores against 0
early_rps_lvl1 <- exp2_dat2_6 %>% filter(game.f=="RPS")  %>% filter(opp_type=="Level_1")
early_rps_lvl2 <- exp2_dat2_6 %>% filter(game.f=="RPS")  %>% filter(opp_type=="Level_2")

t.test(early_rps_lvl1$early_score, mu = 0, alternative = "two.sided")
t.test(early_rps_lvl2$early_score, mu = 0, alternative = "two.sided")

t.test(filter(exp2_dat2_6, game.f == "RPS")$early_score)
```

```{r, include=FALSE}
# Testing FWG early scores against 0
early_fwg_lvl1 <- exp2_dat2_6 %>% filter(game.f=="FWG")  %>% filter(opp_type=="Level_1")
early_fwg_lvl2 <- exp2_dat2_6 %>% filter(game.f=="FWG")  %>% filter(opp_type=="Level_2")

t.test(early_fwg_lvl1$early_score, mu = 0, alternative = "two.sided")
t.test(early_fwg_lvl2$early_score, mu = 0, alternative = "two.sided")
```

```{r, include=FALSE}
# Testing SHOOT early scores against 0
early_shoot_lvl1 <- exp2_dat2_6 %>% filter(game.f=="SHOOT")  %>% filter(opp_type=="Level_1")
early_shoot_lvl2 <- exp2_dat2_6 %>% filter(game.f=="SHOOT")  %>% filter(opp_type=="Level_2")

t.test(early_shoot_lvl1$early_score, mu = 0, alternative = "two.sided")
t.test(early_shoot_lvl2$early_score, mu = 0, alternative = "two.sided")
```

```{r, include=FALSE}
# Set contrasts
contrasts(exp2_dat2_6$game.f) <- cbind(c(1,-1,0),c(0,-1,1))
contrasts(exp2_dat2_6$opp_type) <- c(1,-1)
contrasts(exp2_dat2_6$condition.f) <- c(1,-1)

# Random intercept for participants
lme_early2 <- mixed(early_score ~ game.f*opp_type*block + (1 | human_id),data=exp2_dat2_6,check_contrasts = FALSE, type = 3, method = "KR")
summary(lme_early2)

lme_early2


lsmeans(lme_early2, pairwise~game.f*opp_type, adjust="holm")
# lsmeans(lme_early2, pairwise~game.f, adjust="holm")


```
<!-- We performed a two (Condition: level-1 first, level-2 first) by two (Opponent type: level-1 or level-2) by three (Game: RPS, FWG, Shootout) repeated-measures ANOVA, with the first factor varying between participants. This showed a main effect of Game ($F(1.94,92.98) = 7.67$, $\eta^{2} = 0.04$,  $p < .001$). **MS: Again, if there is no interaction, there might not be justification for running tests per opponent and game ** --> 

<!-- We then ran statistical tests on early round scores by game and opponent against the null hypothesis of 0 (no transfer). For level-1 facing players, there is evidence of learning transfer from RPS to both FWG ($t(271) = 4.99$, $p < .001$) and Shootout ($t(271) = 6.66$, $p < .001$). For level-2 facing players, there is evidence for transfer from RPS to the similar game FWG, albeit scores are lower than for level-1 player ($t(271) = 4.41$, $p < .001$). Unlike in Experiment 1, there is also evidence of transfer to the dissimilar Shootout game ($t(271) = 3.22$, $p = .004$). The score of the players facing the level-1 opponent in Shootout is higher than that of players facing level-2 opponents ($t(144) = 2.45$, $p = 0.01$).-->

<!-- The early round scores in the similar FWG games for level-1 and level-2 facing players are not significantly different from each other. -->

<!--
## Discussion
The results of Experiment 2 confirm earlier findings from Experiment 1 on learning transfer in a situation where participants need to learn about two distinct opponents. As in Experiment 1, participants adapted their strategies to both level-1 and level-2 opponents to exploit deviations from Nash-optimal play. Moreover, we found initial evidence that knowledge about both types of opponents was transferred to both the similar FWG and dissimilar Shootout game. <!-- This transfer was moderated by the degree of similarity between games, as well as opponent type. The interaction effect showed that transfer to the dissimilar game was easier when facing the less sophisticated agent.-->

# Computational modelling

The results of Experiment 2 confirm findings from Experiment 1 on learning transfer in a situation where participants need to learn about two distinct opponents. In both Experiment 1 and 2, participants adapted their strategies to level-1 and level-2 opponents to exploit deviations from Nash-optimal play. Moreover, from early-round performance, we found initial evidence that knowledge about both types of opponents was transferred to similar (FWG) and dissimilar (Numbers or Shootout) games. 

To gain more insight into how participants adapted their strategies to their computer opponents, we constructed and tested several computational models of strategy learning. The baseline model assumes play is random, and each potential action is chosen with equal probability. Note that this corresponds to the Nash equilibrium strategy. The other models adapted their play to the opponent, either by reinforcing successful actions in each game (reinforcement learning), or by determining the type of opponent through Bayesian learning (Bayesian Cognitive Hierarchy models). We also include the (self-tuning) Expected Weighted Attraction (EWA) model, which is a popular model in behavioural economics.

In the following, we will describe the models in more detail, and provide some intuition into how they implement learning about the game and/or the opponent. Throughout, we use the following notation: In each game $g \in \{\text{RPS},\text{FWG}, \text{Numbers}, \text{Shootout} \}$, on each trial $t$, the participant chooses an action $a_t \in \mathcal{A}_g$, and the opponent chooses action $o_t \in \mathcal{A}_g$, where $\mathcal{A}_g$ is the set of allowed actions in game $g$, e.g. $\mathcal{A}_\text{RPS} = \{R,P,S\}$. The participant then receives reward $r_t \in \{1,0,-1\}$, and the opponent receives $-r_t$. We use the state variable $s_t = \{a_{t-1},o_{t-1}\}$ to denote the actions taken in the previous round $t-1$ by the participant and opponent. The initial state is empty, and we assume that the action at a first encounter of an opponent in a game is chosen at random.

## Reinforcement learning (RL) model
We first consider a model-free reinforcement learning algorithm, where actions that have led to positive rewards are reinforced, and the likelihood of actions that led to a negative reward is lowered. Since the computer players in this experiment based their play on the actions in the previous round, a suitable RL model for this situation is one which learns the value of actions contingent on plays in the previous round, i.e. by defining the state $s_{t}$ as above. The resulting RL model learns a $Q$-value [@watkins1992q] for each state-action pair:
$$Q_{t+1}(s_{t},a_{t}) = Q_{t}(s_{t},a_{t}) + \alpha \left( r_{t}  - Q(s_{t},a_{t}) \right) ,$$
where $Q(s_{t},a_{t})$ is the value of taking action $a$ when in state $s$ at time $t$, and $\alpha \in [0,1]$ the learning rate. For instance, $Q_t(\{R,S\},P)$ denotes the value of taking action  "Paper" this round if the player's last action was "Rock" and the opponent played "Scissors". Actions are taken according to a softmax rule:
\begin{equation}
\label{eq:softmax}
P_{t}(a|s_t) = \frac{\exp \{ \lambda Q_{t}(a,s_t) \}}{\sum_{a' \in \mathcal{A}_g} \exp \{\lambda  Q_{t}(a',s_t) \}}, 
\end{equation}
where the inverse temperature parameter $\lambda$ determines the consistency of the strategy (the higher $\lambda$, the more often the action with the highest $Q$-value is chosen). While this RL model allows the players to compute the values of actions conditional on past play, crucially, it will not be able to transfer learning between games, as each game has a different state space $\mathcal{S}_g$ and action space $\mathcal{A}_g$, and there is no simple way to map states and actions across games. The RL model has two free parameters: the learning rate ($\alpha$) and the inverse temperature ($\lambda$).


## Experience-weighted attraction (EWA) model
As discussed in the introductory section on computational models, the self-tuning Experience Weighted Attraction (EWA) model combines two seemingly different approaches, namely reinforcement learning and belief learning. The EWA model is based on updating ``Attractions'' for each action over time given a particular state. The attraction of action $a$ at time $t$ given state $s$ is denoted as $Q_{t}(a, s)$ and updated as 
$$ Q_{t+1}(a,s) =  \frac{\phi(t) \ N(t) \ Q_{t}(a,s) + [ \delta_{a}(t) + (1-\delta_{a}(t)) \ I(a_t = a )] \ R(a,o_t) } {\phi(t)N(t) + 1} ,$$
where $I(x)$ is an indicator function which takes the value 1 if its argument is true and 0 otherwise, and $R(a,o_t)$ is the reward that would be obtained from playing action $a$ against opponent action $o_t$. $R(a,o_t)$ equals the actual obtained reward when $a = a_t$, and otherwise is a counterfactual reward that would have been obtained if a different action were taken. Unlike reinforcement learning, this uses knowledge of the rules of the game to allow reinforcing actions that were not actually taken by the rewards they would have provided. The parameter $\delta$ reflects the weight given to such counterfactual rewards. Setting $\delta = 0$ leads to reinforcement only of actions taken, while positive values of $\delta$ makes the update rule take into account foregone payoffs, which is similar to weighted fictitious play [@cheung1994learning]. $N(t)$ represents an experience weight and can be interpreted as the number of "observation-equivalents" of past experience. We initialise it to 1 so initial attractions and reinforcement from payoffs are weighted equally.

In the earlier version of the EWA model [@camerer1997experience], $\phi$ and $\delta$ were time-invariant free parameters. In the self-tuning EWA model [@ho2004economics], the values of $\delta$ and $\phi$ are learnt from experience. Over time, $\delta$ is updated as
$$\delta_{a}(t) = \begin{cases} 1 & \text{if }  R(a,o_{t}) \geq r_{t}  \\
0 & \text{otherwise} \end{cases}$$ 
The $\phi_{t}$ parameter can be interpreted as a discount of prior experience, modelling either limited agent memory or changes in the game conditions. At the core, $\phi(t)$ depends on a surprise index $S_{p}(t)$: 
$$\phi(t) = 1 - \frac{1}{2}S_{p}(t) ,$$
where $S_{p}(t)$ quantifies how the opponent deviates from past play. It is calculated in turn through the cumulative history of play, across opponent possible actions k, as $h^{k}(t)$:
$$h^{k}(t)= \frac{ \sum_{\tau = 1}^t  I( o_{\tau} = o^k )} {t}, $$ 
as well as an indicator vector of the most recent opponent play:
$$r^k(t) = I(o^k=o_{t}), $$ 
Where $I$ is the indicator function as defined above. To get the surprise index, we simply sum all the squared deviations between the cumulative history vector $h^{k}(t)$ and the immediate history $r^k(t)$:
$$S_{p}(t) = \sum_{k=1}^{|\mathcal{A}_g|} (h^{k}(t) - r^k(t))^2 $$ 
For more details on the self-tuning EWA model, we refer the reader to [@ho2004economics]. As in the RL model above, actions are chosen based on a softmax decision rule (Equation \ref{eq:softmax}). 
<!-- 
$$P_t(a,s) = \frac{\exp \{\lambda  Q_{t}(a,s) \} }{\sum_{a' \in \mathcal{A}_t} \exp \{ \lambda Q_{t}(a',s) \} } $$
-->
The self-tuning EWA has one free parameter: the inverse temperature of the softmax decision rule ($\lambda$). 

## Bayesian Cognitive Hierarchy (BCH) model

In what we call the Bayesian Cognitive Hierarchy (BCH) model, the participant attempts to learn the type of opponent they are facing through Bayesian learning. For present purposes, we assume participants consider the opponent could be either a level 0, level 1, or level 2 player, and start with a prior belief that each of these types is equally likely. They then use observations of the opponent's actions to infer a posterior probability of each type:
$$P(\text{level}=k | \mathcal{D}_{t})  \propto  P(\mathcal{D}_{t}|\text{level}=k ) \times P(\text{level}=k) ,$$
where $\mathcal{D}_{t} = \{s_1,\ldots,s_t\}$ is the data available at time $t$. The likelihood is defined as
$$P(\mathcal{D}_{t}|\text{level}=k) = \prod_{j=1}^t \left( \theta \frac{1}{|\mathcal{A}_g|} + (1-\theta) f_k(o_j|s_{j})\right) ,$$
where $f_k(o_t|s_{t}) = 1$ if $o_t$ is the action taken by a level $k$ player when the previous round play was $s_t = (a_{t-1}, o_{t-1})$, and 0 otherwise. Note that the likelihood assumes (correctly) that there is a probability $\theta \in [0,1]$ that the opponent takes a random action. The posterior at time $t-1$ forms the prior at time $t$. The probability that an action $a$ is the best response on trial $t$ is defined as:
$$B_t(a) = \sum_{k = 0}^2 \sum_{o \in \mathcal{A}_g} b(a,o) f_k(o|s_{t})  P(\text{level}=k|\mathcal{D}_{t-1}), $$
where $b(a,o) = 1$ if action $a$ is a best response to opponent's action $o$ (i.e. it leads to a win). 

We assume participants choose actions through a softmax over these probabilities of best responses:
$$P_t(a|s_t) = \frac{\exp\{\lambda B_t(a) \}}{\sum_{a' \in \mathcal{A}_g} \exp \{ \lambda B_t(a')\}}.$$
where $b(a,o) = 1$ if action $a$ is a best response to opponent's action $o$ (i.e. it leads to a win).

Unlike the models above, the BCH model allows for between-game transfer, as knowledge of the level of the opponent can be used to generate predictions in games that have not been played before. This generalization is done simply by using the posterior $P(\text{level} = k|\mathcal{D}_T)$ from the previous game as the prior distribution in the next game. However, the participant might also assume that the level of reasoning of their opponent does not generalize over games. This would mean starting with a "fresh" prior $P(\text{level} = k)$ at the start of each game. We hence distinguish between two versions of the BCH model. In the No-Between-Transfer (BCH\_NBT) variant, participants assume a uniform probability of the different levels at the start of each game (and hence do not transfer knowledge of their opponent between games). In the Between-Transfer model (BCH\_BT), participants use the posterior probability over the levels of their opponent as the prior at the start of a new game (i.e. complete transfer of the knowledge of their opponent). Both versions of the BCH model have two free parameters: the assumed probability that the opponent chooses a random action ($\theta$), and the temperature parameter of the softmax function ($\lambda$). 

## Estimation
For both experiments, we fitted all models to individual participant data by maximum likelihood estimation using the DEoptim R package [@R-DEoptim]. We use the Bayesian Information Criterion (BIC) to determine the best fitting model for each participant. 
For Experiment 1, we fitted a total of 5 models: a baseline model assuming random play (Nash), the Bayesian Cognitive Hierarchy model allowing transfer between games (BCH\_BT) and without transfer between games (BCH\_NT), as well as a reinforcement learning (RL), and finally a self-tuning EWA model with the same state space (EWA).
In Experiment 2, because participants were interacting with each opponent twice within each game, we need to distinguish between two types of opponent model transfer. We can have transfer *within* games between the first and second interaction with the opponent. In addition, we can also have transfer *between* games, as in e.g. transferring a learned opponent model from RPS to FWG. Therefore, we fitted a total of three versions of the Bayesian Cognitive Hierarchy model: BCH\_BT allows for both within and between game transfer (between game transfer without within game transfer is implausible); BCH\_WT allows only for within-game transfer, but not between game transfer; BCH\_NT allows for no transfer within or between games. As the RL model can't account for between game transfer due to a change in state and action space, we can only have models that allowing for within game transfer (RL\_WT) or with no transfer within games (RL\_NT). Likewise, we fit both a self tuning EWA model with (EWA\_WT) and without (EWA\_NT) within-game transfer. Including the baseline model of random play (Nash), we therefore fitted a total of 8 models for Experiment 2. 


```{r load-comp-mod-distrib, include=FALSE}

exp1_comp_results <- read.csv(file="../exp1_all_results.csv")
exp1_comp_table <- table(exp1_comp_results[, "condition"],c("Nash","BCH_BT","BCH_NT", "RL","EWA")[apply(exp1_comp_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,which.min)])

 #write.csv(exp1_comp_table ,file="exp1_comp_table ",row.names = TRUE)

```

## Model comparison

When considering which single model fits participants behaviour over the whole experiment best, we find that in both experiments the RL model outperforms the other models, followed by the random (Nash) model (Figure \ref{fig:exp-comp-models}). Relatively few participants were best described by one of the BCH models or the EWA model. <!-- We have also estimated BIC weights for each model and we confirm that <!-- (Figure \ \@ref(fig:xp1-BIC-weigths)), we confirm this . RL models have high BIC weights when they best fit the participants, and very few instances have high BIC weights for models other then RL, which fits the picture drawn by the histogram.  **IG: mentioned we looked at BIC weights and removed BIC wgt plot ** -->

```{r exp-comp-models, fig.cap = "Number of participants best fitted by each model  in Experiment 1 and 2 according to the BIC. Results are seperated by opponent (Experiment 1) and which opponent they faced first (Experiment 2).", out.width="\\textwidth", fig.align='center'}

#library(gridExtra)

#plots <- list()

exp1_comp_df <- as.data.frame(exp1_comp_table)
colnames(exp1_comp_df) <- c("Opponent","model","Freq")


exp1_comp_df$model.f <- factor(exp1_comp_df$model, levels = c("Nash","RL","EWA", "BCH_NT", "BCH_BT"), ordered =TRUE)

exp1_comp_df$experiment <- "Experiment 1"

#plots[[1]] <- ggplot(data = exp1_comp_df,aes(x = model.f, y = Freq, fill = Opponent)) + geom_bar(stat = "identity") + xlab("Best fitting model") + ylab("Number of participants") + theme_apa() + ggtitle("Experiment 1") + theme(legend.position = "bottom", axis.text.x = element_text(angle=90)) # + coord_flip()



exp2_comp_results <- read.csv(file="../Experiment_2/exp2_results.csv")

exp2_comp_table <- table(exp2_comp_results[, "condition"],c("Nash","BCH_BT","BCH_WT", "BCH_NT", "RL_NT","RL_WT","EWA_NT","EWA_WT")[apply(exp2_comp_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,which.min)])

 exp2_comp_df <- as.data.frame(exp2_comp_table)
colnames(exp2_comp_df) <- c("Opponent","model","Freq")

exp2_comp_df$model.f <- factor(exp2_comp_df$model, levels = c("Nash","RL_NT","RL_WT","EWA_NT","EWA_WT","BCH_NT", "BCH_WT","BCH_BT"), ordered =TRUE)

exp2_comp_df$experiment <- "Experiment 2"

exp_comp_df <- rbind(exp1_comp_df, exp2_comp_df)
exp_comp_df$model.f <- factor(exp_comp_df$model.f, levels=c("Nash","RL","RL_NT","RL_WT","EWA","EWA_NT","EWA_WT","BCH_NT", "BCH_WT","BCH_BT"))

ggplot(data = exp_comp_df,aes(x = model.f, y = Freq, fill = Opponent)) + geom_bar(stat = "identity") + xlab("Best fitting model") + ylab("Number of participants") + theme_apa() + labs(fill = "Opponent (faced first)") + theme(legend.position = "top", axis.text.x = element_text(angle=90)) + facet_wrap(~experiment, scales = "free_x")

#plots[[2]] <- ggplot(data = exp2_comp_df,aes(x = model.f, y = Freq, fill = condition)) +geom_bar(stat = "identity") + xlab("Best fitting model") + ylab("Number of participants") + theme_apa() + labs(fill = "Opponent faced first") + ggtitle("Experiment 2") + theme(legend.position = "top", axis.text.x = element_text(angle=90))

#grid.arrange(plots[[1]], plots[[2]], ncol=2)

```


As the RL models don't transfer learning between games, this is at odds with the behavioural results, which showed evidence for such transfer. 
<!--
, where we found evidence of transfer in **early rounds** of each game. If indeed most participants adopt a reinforcement learning strategy, they should not be able to transfer their learning to the early rounds of a new game. --> In order to investigate this discrepancy further, we plot the likelihood by trial for each game and three strategies: reinforcement learning (RL), Bayesian Cognitive Hierarchy with between-game transfer (BCH\_BT), and the random (Nash) strategy. Figure \ref{fig:exp1-lik-by-tr} shows that in the initial rounds of the the later games in Experiment 1, the likelihood for the BCH model is higher than that of the other two models. However, over time, the likelihood of the RL model increases and exceeds that of the BCH model. The same pattern holds for Experiment 2 (Figure \ref{fig:exp2-lik-by-tr}). Again, the BCH\_BT model with between-game transfer has the highest likelihood in the early stages of the later games (apart from stage 1 of the Shootout game, where the RL model is better). In later rounds, the likelihood of the RL model exceeds that of the BCH model. 

The fact that the likelihoods of the main strategies considered cross over in both experiments is consistent with participants switching between strategies as the games progress. According to this interpretation, participants base their responses in early rounds on the learned level of their opponents iterative reasoning, switching later to learning good actions through reinforcement. **We initially did not predict this strategy switching. Whilst the likelihoods are suggestive, we sought an approach to formally test for strategy switching.**


<!-- In Experiment 2, we can see from Figure \ref{fig:exp2-comp-models} that the RL model was again more successful than the Bayesian Cognitive Hierarchy models (with or without transfer) and the EWA model in fitting participants' action choices. <!--  In Experiment 2 when participants faced both level-1 and level-2 agents sequentially, the Bayesian models (with or without transfer) did not fit players observed data as well. --> 

<!-- This is also reflected in BIC weights in Figure \@ref(fig:xp2-BIC-weigths). -->




<!-- Plotting cumulative scores by best model for experiment 2, we see very similar results looking at Figure\ \@ref(fig:exp2-cumScores), in that participants whose behavior was best fit by a BCH model of learning the opponent strategy had the highest cumulative performance. Out of these BCH models, the one in which there is within-game but no between-game transfer (NBT) had the best cumulative performance (although it only fit 2 participants best), followed by a model in which both within and between transfer of opponent models is allowed (BT). The next best model from a performance perspective was a Q-learning model with states and within game transfer, followed by BCH models where players reset opponent models at each stage of each game (NT). As expected, random play was at the bottom of cumulative performance. -->

## Hidden Markov model analysis of strategy switching

<!-- **When using data on all rounds**, the computational modelling indicates that most players are best fit by a reinforcement learning which learns good actions conditional upon the last round play.  This is at odds with the behavioural findings, where we found evidence of transfer in **early rounds** of each game. If indeed most participants adopt a reinforcement learning strategy, they should not be able to transfer their learning to the early rounds of a new game. In order to investigate this discrepancy further, we plot the likelihood by trial for each game and three strategies: reinforcement learning (RL), Bayesian Cognitive Hierarchy with between-game transfer (BCH\_BT), and the random (Nash) strategy. Figure \ref{fig:exp1-lik-by-tr} shows that in the initial rounds of the the later games in Experiment 1, the likelihood for the BCH models is higher than that of the other models. However, over time, the likelihood of the RL model increases and exceeds that of the BCH model. -->

```{r, include= FALSE}

exp1_lik_by_trial  <- read.csv("../exp1_lik_by_trial.csv")

exp1_lik_by_trial <- exp1_lik_by_trial %>%
    dplyr::mutate(Nash_lik = ifelse( (game == "rps" | game == "fwg"),1/3, 1/5))

mean_lik <- exp1_lik_by_trial %>%
  dplyr::select(human_id,condition,game, round, Nash_lik, QLS_lik, Bayes_Tr_lik) %>%
  group_by(condition,game,round) %>%
  dplyr::summarise(mean_Nash_lik = mean(Nash_lik, na.rm = TRUE),
                   mean_QLS_lik = mean(QLS_lik, na.rm = TRUE),
                   mean_Tr_lik = mean(Bayes_Tr_lik, na.rm = TRUE))

mean_lik$game <- factor(mean_lik$game,levels=c("rps","fwg","numbers"))

data_long <- tidyr::gather(mean_lik, strategy, probability, mean_Nash_lik:mean_Tr_lik, factor_key=TRUE)


```

```{r exp1-lik-by-tr, fig.cap = "Average likelihood of the random (Nash), reinforcement learning (RL) and Bayesian Cognitive Hierarchy model by trial, game, and opponent faced in Experiment 1.", fig.align='center'}
# make sure the different games are ordered in the way they were played
# data_long$game <- factor(data_long$game,levels=c("rps","fwg","numbers"))
data_long %>%
  mutate(condition = factor(condition)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(condition = forcats::fct_recode(condition, `Level 1` = "Level1", `Level 2` = "Level2")) %>%
ggplot(aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~condition) +
  scale_colour_discrete(name = "Strategy", labels = c("Nash", "RL", "BCH_BT")) + theme_apa()

```

<!-- The same pattern holds for Experiment 2 (Figure \ref{fig:exp2-lik-by-tr}). Again, the BCH\_BT model with between-game transfer has the highest likelihood in the early stages of the later games (apart from stage 1 of the shootout game, where the RL model is better). In later rounds, the likelihood of the RL model exceeds that of the BCH model. <!-- second (most similar) game, however the likelihood of Q-learning with states models increases steadily to be the highest in the later stages of all games. In the third and more dissimilar game, we get a result that is different from experiment 1. In this instance, the likelihoods of the BCH models stay constant and close to their initial values.-->


```{r, include = FALSE }

exp2_lik_by_trial <- read.csv("../Experiment_2/exp2_lik_by_trial.csv")

exp2_lik_by_trial <-  exp2_lik_by_trial %>% dplyr::mutate(Nash_lik = 1/3)

mean_lik_2 <- exp2_lik_by_trial %>%
  dplyr::select(human_id,game,stage,round_condition, round, Nash_lik, QLS_lik_within, Bayes_Tr_lik) %>%
  dplyr::group_by(game,stage,round) %>%
  dplyr::summarise(mean_Nash_lik = mean(Nash_lik, na.rm = TRUE),
             mean_QLS_within_lik = mean(QLS_lik_within, na.rm = TRUE),
             mean_BCH_Tr_lik = mean(Bayes_Tr_lik, na.rm = TRUE))

mean_lik_2$game <- factor(mean_lik_2$game,levels=c("rps","fwg","shootout"))

data_long2 <- gather(mean_lik_2, strategy, probability, mean_Nash_lik:mean_BCH_Tr_lik, factor_key=TRUE)
```

```{r exp2-lik-by-tr, fig.cap = "Average likelihood of the random (Nash), reinforcement learning (RL) and Bayesian Cognitive Hierarchy model by trial, stage, and game in Experiment 2.", fig.align ='center'}
# make sure the different games are ordered in the way they were played
# data_long2$game <- factor(data_long$game,levels=c("rps","fwg","shootout"))
data_long2 %>%
  mutate(stage = paste("Stage",stage)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Shootout = "shootout")) %>%
ggplot(aes(x=round,y=probability,colour=strategy)) +
  #mutate(stage = forcats::fct_recode(stage, `Level 1` = "Level1", `Level 2` = "Level2")) %>%
  geom_line() +
  facet_grid(game~stage) +
  scale_colour_discrete(name = "Strategy", labels = c( "Nash", "RL_WT", "BCH_BT")) + theme_apa()

```


We use hidden Markov models to test for strategy switching in participants' play. In these models, the three strategies (RL, BCH with between-game transfer, and Nash) correspond to latent states which determine the overt responses (actions chosen). The models allow for switching between the states (i.e. strategies) over time. Hidden Markov models assume that an observable action at time $t$ depends on a latent state at time $t$. It is assumed that the latent state at time $t$ depends on the latent state at the previous time $t-1$. The model is specified by the state-conditional action distributions (these are provided by the likelihood of the fitted models), an initial state distribution (the distribution over the strategies at the initial round), and the state-transition probabilities (probability of switching from one state/strategy to another). Initial state probabilities and the transition probabilities were estimated with the depmixS4 package [@R-depmixS4]. As a statistical test of strategy switching, we compare the hidden Markov model to a constrained version which assumes the probability of switching from one strategy to a different one is 0. This model thus assumes that when players start with a particular strategy, they continue using it throughout the experiment.


```{r define-dummy-response, include=FALSE}
setClass("dummyResponse", contains="response")

setGeneric("dummyResponse", function(y, pstart = NULL, fixed = NULL, ...) standardGeneric("dummyResponse"))

setMethod("dummyResponse", 
    signature(y="ANY"), 
    function(y,pstart=NULL,fixed=NULL, ...) {
      y <- matrix(y,length(y))
  		x <- matrix(1)
  		parameters <- list()
  		npar <- 0
      mod <- new("dummyResponse",parameters=parameters,fixed=logical(0),x=x,y=y,npar=npar)
      mod
	}
)

setMethod("show","dummyResponse",
    function(object) {
        cat("Dummy for fixed likelihood Model \n")
    }
)

setMethod("dens","dummyResponse",
  function(object,log=FALSE) {
   if(log) log(as.numeric(object@y)) else as.numeric(object@y)
  }
)

setMethod("getpars","dummyResponse",
    function(object,which="pars",...) {
        switch(which,
            "pars" = {
                pars <- numeric(0)
            },
            "fixed" = {
                pars <- logical(0)
            }
        )
        return(pars)
    }
)

setMethod("setpars","dummyResponse",
    function(object, values, which="pars", ...) {
        npar <- npar(object)
        if(length(values)!=npar) stop("length of 'values' must be",npar)
        # determine whether parameters or fixed constraints are being set
		nms <- ""
		switch(which,
		  "pars"= {
		      },
		  "fixed" = {
		    }
		  )
      names(object@parameters) <- nms
      return(object)
    }
)

setMethod("fit","dummyResponse",
    function(object,w) {
		  return(object)
	}
)

setMethod("predict","dummyResponse", 
    function(object) {
        ret <- object@y
        return(ret)
    }
)
```

```{r, include=FALSE}

## EXPERIMENT 1 Data 

Nash_lik <- exp1_lik_by_trial$Nash_lik
RL_lik <- exp1_lik_by_trial$QLS_lik
Bayes_Tr_lik <- exp1_lik_by_trial$Bayes_Tr_lik

nsubject <- length(unique(exp1_lik_by_trial$human_id)) # number of participants
ngame <- 3 # number of games
ntrial <- c(50,50,50) # numer of trials in each game

rModels <- list(
  list(
	  dummyResponse(RL_lik)
	),
	list(
		dummyResponse(Nash_lik)
	),
  list(
    dummyResponse(Bayes_Tr_lik)
  )
)

trstart <- matrix(c(0.8,0.1,0.1,0.1,0.8,0.1,.1,.1,.8),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

instart <- c(1/3,1/3,1/3)
inMod <- transInit(~1,nstates=3,pstart=instart,family=multinomial("identity"),data=data.frame(rep(1,nsubject*ngame)))

mod1 <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod1 <- fit(mod1, emcontrol=em.control(random.start=FALSE))
```


```{r no-switch, include=FALSE}

# No switching. Force off diagonal initial elements of transtion matrix to 0
trstart <- matrix(c(1,0,0,0,1,0,0,0,1),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

mod1_noswitch <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod1_noswitch <- fit(mod1_noswitch, emcontrol=em.control(random.start=FALSE))

# p-value for comparison between a model with strategy switches and one without:

llratio(fmod1,fmod1_noswitch)

1-pchisq(-2*as.numeric(logLik(fmod1_noswitch)) - (-2*as.numeric(logLik(fmod1))),df=6)
```

```{r, include = FALSE}

exp1_lik_by_trial["post_RL"] <- forwardbackward(fmod1)$gamma[,1]
exp1_lik_by_trial["post_Nash"] <- forwardbackward(fmod1)$gamma[,2]
exp1_lik_by_trial["post_Bayes_Tr"] <- forwardbackward(fmod1)$gamma[,3]

mean_post_1 <- exp1_lik_by_trial %>%
  dplyr::select(human_id,game,condition, round, post_Nash, post_RL, post_Bayes_Tr) %>%
  dplyr::group_by(game,condition, round) %>%
  dplyr::summarise(Nash = mean(post_Nash, na.rm = TRUE),
                   RL_WT = mean(post_RL, na.rm = TRUE),
            BCH_BT = mean(post_Bayes_Tr, na.rm = TRUE))

mean_post_1$game <- factor(mean_post_1$game,levels=c("rps","fwg","numbers"))

data_long_post1 <- gather(mean_post_1, strategy, probability, Nash, RL_WT, BCH_BT, factor_key=TRUE)


```

```{r exp1-posteriors-plot, fig.cap = "Average posterior probability of each strategy by trial, game and opponent faced, in Experiment 1.", fig.align ='center'}
data_long_post1 %>%
   mutate(condition = factor(condition)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(condition = forcats::fct_recode(condition, `Level 1` = "Level1", `Level 2` = "Level2")) %>%
ggplot(aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~condition) +
  ylab("posterior probability") + theme_apa()

```

In Experiment 1, a likelihood-ratio test shows that the HMM model with switching fits significantly better than the non-switching one, $\chi^2(6) = `r round(-2*as.numeric(logLik(fmod1_noswitch)) - (-2*as.numeric(logLik(fmod1))),2)`$, $p < .001$. We should note that as the no switch model involves restricting parameters of the switching model on the bounds of the parameter space, the $p$-value of this test may not correspond to the true Type-1 error rate. We therefore also consider model selection criteria. These also show that the switching model (AIC = `r prettyNum(-2*as.numeric(logLik(fmod1)) + 2*8)`, BIC = `r prettyNum(-2*as.numeric(logLik(fmod1)) + 8*log(nrow(fmod1@dens)))`) fits better than the non-switching model (AIC = `r prettyNum(-2*as.numeric(logLik(fmod1_noswitch)) + 2*2)`, BIC = `r prettyNum(-2*as.numeric(logLik(fmod1_noswitch)) + 2*log(nrow(fmod1_noswitch@dens)))`). <!-- **MS: we should really use a bootstrapped Likelihood Ratio test here ** --> This provides further statistical evidence in favour of the hypothesis that participants switch between strategies. Figure \ref{fig:exp1-posteriors-plot} depicts the average posterior probability of each strategy, as a function of trial and opponent faced. As can be seen, there is evidence of strategy switching in the FWG and Numbers games: Initially, participants appear to use a random strategy (in the first round of a game, there is no way to predict the opponent's action), after which the BCH strategy becomes dominant. In the later rounds of the games, the RL strategy becomes dominant, however. 

```{r, include =FALSE}
## EXPERIMENT 2 DATA 
# we need to have "dat" available and lik_by_trial
RL_lik <- exp2_lik_by_trial$QLS_lik
Nash_lik <- exp2_lik_by_trial$Nash_lik
Bayes_Tr_lik <- exp2_lik_by_trial$Bayes_Tr_lik

nsubject <- length(unique(exp2_lik_by_trial$human_id)) # number of participants
ngame <- 12 # number of games
#ntrial <- c(60,60,60)

ntrial <- c(20,20,10,10,20,20,10,10,20,20,10,10) # number of trials in each game

rModels <- list(
  list(
	  dummyResponse(RL_lik)
	),
	list(
		dummyResponse(Nash_lik)
	),
  list(
    dummyResponse(Bayes_Tr_lik)
  )
)

trstart <- matrix(c(0.8,0.1,0.1,0.1,0.8,0.1,.1,.1,.8),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

instart <- c(1/3,1/3,1/3)
inMod <- transInit(~1,nstates=3,pstart=instart,family=multinomial("identity"),data=data.frame(rep(1,nsubject*ngame)))

mod <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod2 <- fit(mod, emcontrol=em.control(random.start=TRUE))

# No switching. Force off diagonal initial elements of transtion matrix to 0
trstart <- matrix(c(1,0,0,0,1,0,0,0,1),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

mod2_noswitch <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod2_noswitch <- fit(mod2_noswitch, emcontrol=em.control(random.start=FALSE))


```

```{r, include=FALSE}

exp2_lik_by_trial["post_RLw"] <- forwardbackward(fmod2)$gamma[,1]
exp2_lik_by_trial["post_Nash"] <- forwardbackward(fmod2)$gamma[,2]
exp2_lik_by_trial["post_Bayes_Tr"] <- forwardbackward(fmod2)$gamma[,3]

mean_post_2 <- exp2_lik_by_trial %>%
  dplyr::select(human_id,game,stage,round_condition, round, post_Nash, post_RLw, post_Bayes_Tr) %>%
  dplyr::group_by(game,stage, round) %>%
  dplyr::summarise(Nash = mean(post_Nash, na.rm = TRUE),
                   RL = mean(post_RLw, na.rm = TRUE),
            BCH_BT = mean(post_Bayes_Tr, na.rm = TRUE))

mean_post_2$game <- factor(mean_post_2$game,levels=c("rps","fwg","shootout"))

data_long_post2 <- gather(mean_post_2, strategy, probability, Nash, RL, BCH_BT, factor_key=TRUE)


```

```{r exp2-posteriors-plot, fig.cap = "Average posterior probability of each strategy by trial, stage, and game in Experiment 2.", fig.align ='center'}
data_long_post2 %>%
  mutate(stage = paste("Stage",stage)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Shootout = "shootout")) %>%
 ggplot(aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~stage) +
  ylab("posterior probability") + theme_apa()

```

In Experiment 2, the switching model (AIC = `r prettyNum(-2*as.numeric(logLik(fmod2)) + 2*8)`, BIC = `r prettyNum(-2*as.numeric(logLik(fmod2)) + 8*log(nrow(fmod2@dens)))`) again fits better than the restricted non-switching model (AIC = `r prettyNum(-2*as.numeric(logLik(fmod2_noswitch)) + 2*2)`, BIC = `r prettyNum(-2*as.numeric(logLik(fmod2_noswitch)) + 2*log(nrow(fmod2_noswitch@dens)))`), $\chi^2(6) = `r round(-2*as.numeric(logLik(fmod2_noswitch)) - (-2*as.numeric(logLik(fmod2))),2)`$, $p < .001$. The posterior probabilities of the strategies (Figure \ref{fig:exp2-posteriors-plot}) show very clear evidence of strategy switching across games and stages, from using a BCH model in the initial rounds to an RL strategy later on.

# Discussion

In this study, we investigated human learning transfer across games by making human participants play against computer agents with limited levels of iterated reasoning. We were interested in whether participants learn about the strategy of their opponent and transfer such knowledge between games. <!--, and whether this is modulated by the similarity between games and the sophistication of the agent.-->

The results of our first experiment show that the majority of participants learnt to adapt to their opponent's strategy over multiple rounds of the Rock-Paper-Scissors (RPS) game, and generalised this learning to a similar (Fire-Water-Grass) and less similar (Numbers) game. <!-- Performance in early rounds indicated that learning transfer was moderated by the degree of similarity between games.--> In the second experiment, participants faced both types of opponents, allowing for a stronger test of opponent modelling, as participants would need to learn a different strategy for each opponent. In Experiment 1, participants could learn a single strategy for each game, making opponent modelling possibly less pertinent. In Experiment 2, we again found clear evidence of transfer in the early rounds of the later games. <!-- In addition to the moderation of transfer by the similarity between games, we also found that learning transfer was moderated by the type of opponent faced: Transfer was harder when facing the level-2 opponent as compared to the level-1 opponent. That evidence for transfer was weaker when facing the more sophisticated level-2 opponent may be due to a higher difficulty of learning that opponent's strategy. If it is more difficult to establish a model of the level-2 opponent, there is likely less knowledge to transfer to the new game. A player cannot transfer what they have not learnt.-->

What exactly did the players learn in the first game (RPS) that allowed them to beat their opponent in the later games (FWG, and Numbers or Shootout)? What did the players learn specifically about their opponent's strategy and what form did this learning take? One possible answer is that participants learned simple rules based on last round play. For instance, "play Scissors whenever my opponent played Rock in last round", or "play Paper whenever the last round play was either Rock or Scissors". These are the type of strategies that are learned by the model-free reinforcement learning we used in our computational modelling. While this strategy fitted participants' actions the best overall, there are at least two reasons why this account is not satisfactory as a complete description of participants' behaviour. Firstly, the learned strategies are not transferable to new games. There is no simple way to map "play Scissors whenever my opponent played Rock in last round" in the RPS game to "play Grass whenever my opponent played Fire in last round". Such a mapping may be possible by translating the rules and structure from RPS to FWG, but model-free reinforcement learning lacks the tools to do this: Model-free reinforcement learning would need to start from scratch in each new game, yet we found evidence that participants could successfully exploit their opponent's strategy in early rounds of new games. Secondly, a reinforcement learning strategy would fare equally well against the level-1 and level-2 opponent. Whilst choosing different actions, the contingency between the state (last round play) and actions is the same for both opponents. Yet, in Experiment 1, we found that participants performed better against the level-1 opponent compared to the level-2 opponent. The difference in performance between the two types of opponent indicates that the actions of the more sophisticated level-2 opponent, or the best response to these, were somehow more difficult to predict.

We are left with two possible explanations: First, it is possible that participants discovered a heuristic that allowed them to beat their opponent without explicitly modelling their strategy, and that this heuristic is transferable to new games. Because of the cyclicity in action choices (e.g., Rock beats Scissors, Scissors beats Paper, Paper beats Rock), it is possible to beat a level-2 opponent most of the time by following a simple rule: Play in the next round whatever the opponent played in the last round. This is a rule that wins and is transferable to other games, as it does not depend on action labels. In the same vein, a heuristic that beats a level-1 player can be stated as "Choose the action that would have been beaten by my previous action". Intuitively, it seems that the heuristic for a level-2 player is simpler than that for a level-1 player, which is at odds with the finding that participants performed better against the level-1 opponent.

A second explanation is that participants engaged in iterative reasoning, inferring their opponent's beliefs and countering the resulting actions. For instance, this would be reasoning of the form "My opponent expects me to repeat my last action, choosing an action that would beat my last action. Hence, I will choose the action that beats their best response" or "My opponent thinks I expect them to repeat their action, hence expecting me to choose the action that beats their last action. They will therefore choose the action that beats this one, and hence I should choose the action that beats their best response." Beating a level-1 player, in this account, requires being a level-2 player, and beating a level-2 player requires being a level-3 player. Intuitively, the additional step of iterative reasoning involved in beating a level-2 player makes the level-3 strategy more demanding and difficult to implement, which is consistent with the lower performance against the level-2 opponent.  **We did not find a clear difference in performance against the two opponents in Experiment 2, where participants faced both consecutively. Learning about the distinct strategies of two different opponents is likely more difficult than learning about a single opponent. And switching between opponents may place additional demands on memory and impose task switching costs. These factors may have masked the performance difference found in Experiment 1. Nevertheless, we found that participants learned to successfully exploit the strategies of both players in Experiment 2, as well as transferring their learning to new games.**

The difference in performance against the two opponents, coupled with evidence for successful transfer, point to participants engaging in iterative reasoning and learning something useful about their opponent's limitations in this regard. This is the type of learning encapsulated by our Bayesian Cognitive Hierarchy model. It involves the evaluation of explicit hypotheses and results in better problem-solving skills [@mandler2004foundations]. Since it is less context dependent, this type of learning is generalizable to new situations, akin to the more general framework of rule-based learning explored by Stahl [@stahl2000rule; @stahl2003sophisticated]. When asked to describe the strategy of their opponents at the end of the study, many participants made reference to a form of recursive reasoning, which provides additional support for this account.

We admit that our implementation in the BCH models does not predict a performance difference between the types of opponents. Starting with an equal prior belief over the different levels of sophistication, a BCH player would perform equally well against the level-1 and level-2 opponent. There are two routes to explain the difference in performance. Firstly, prior beliefs might be biased against higher-level opponents (i.e., participants might have believed it is more likely that they would face level-1 opponent than a level-2 opponent). Secondly, if the actions of a level-2 opponent are more difficult to predict than those of a level-1 opponent, this might introduce more noise in the likelihood of the opponents actions given their level of sophistication. Either of these mechanisms would explain why learning the strategy of the level-2 opponent is more difficult and slower than learning the strategy of the level-1 opponent.

Using hidden Markov models, we found evidence of strategy switching between the BCH and RL strategies, and such switching seems more consistent with the latter idea. If predicting an opponent's actions through iterative reasoning is cognitively demanding and error-prone, it is resource-rational to switch to less costly yet equally successful strategies when these are available [@lieder2020resource]. Initially, a model-free reinforcement learning strategy will be less successful than an iterative reasoning one. However, given enough experience, it will be on par with an iterative reasoning strategy. As it involves simple state-action contingencies, a model-free RL strategy may also be computationally less costly, making it overall more effective to rely on this than iterative reasoning. This is similar to the arbitration between model-free and model-based RL [@daw2005uncertainty;@Simon_Daw_11; @Kool_2011]. In repeated and well-practised situations, relying on habits allows one to save cognitive resources for other demands. However, when the environment -- or game -- changes, it is prudent to use all available resources to reorient oneself. 

**A limitation of the design of both experiments is that the order of the games was not counterbalanced. We chose for the order used as we believed it would be most conducive to learning the strategy of the opponent. Starting with a familiar game (RPS) would allow participants to focus on how their opponent plays, rather than also having to learn the rules of the game. A new but isomorphic game (FWG) allows for the most straightforward transfer. We reasoned that transfer to a less similar game (Numbers of Shootout), a stronger test of generalization, would be more likely given sufficient experience in two similar games. Determining the level of iterative reasoning in these latter games is more difficult (the Numbers game has many ties, whilst in Shootout actions are often consistent with different level-$k$ opponents). Counterbalancing the order of the games would mean many participants would start with these more difficult games. Whilst we believe these games are good candidates for testing generalisation of previously learned opponent models, they are poor environments to learn them in the first place. Since participants can't transfer what they have not learned, we expect that starting with these games might prohibit learning transfer. However, we can't rule out that part of the transfer effects found for early round scores is due to general practice effects, or differences in the difficulty of the games. We find it unlikely that the transfer effects we found are entirely due to such confounded practice or difficulty effects. Firstly, general practice or difficulty effects should not depend on the level of the opponent. However, we found evidence that overall scores (Experiment 1) and early round scores (Experiment 2) differed between the two types of opponents. Secondly, practice effects would be evident within a game, but we found no main effect of stage on early-round scores in Experiment 2. **

<!--
, then chose to test generalisation in a dissimilar game. Counterbalancing might lead to participants starting with Numbers or Shootout, which would make learning the level of the opponent harder (Numbers have many uninformative ties and Shootout makes it hard to determine the degree of reasoning since some action predictions are consistent with different level-k opponents). This makes these games good candidates for testing generalisation of previously learned opponent models but poor environments to learn them in the first place. Since participants can't transfer what they have not learned, we expect that starting with these games might prohibit learning transfer.** 

Due to the current design, we can't rule out that part of the transfer effect found in early round scores is due to general practice effects over the games. Nevertheless, it is unlikely that the transfer effects are entirely due to such confounded practice effects. First, general practice effects would not depend on the level of sophistication of the opponent. However, we found that early round scores differed between the two types of opponents. Second, practice effects should be evident within a game and therefore lead to better performance in experiment 2 when the participant meets the second opponent for the first time, compared to meeting the first opponent for the first time. This was not the case.
-->

# Conclusion 

In two experiments we found that people successfully deviate from Nash equilibrium play to exploit deviations from such play by their opponents. Moreover, people can transfer knowledge about the underlying limitations of their opponents' degree of iterative reasoning to new situations. <!-- This transfer of an opponent depends on the similarity between the prior and new game, as well as the sophistication of the opponent. Transfer is better to similar games, and for less sophisticated agents. --> Within games, we found evidence that people start new games with a reasoning-based strategy that allows them to transfer knowledge from other games. After gaining sufficient experience with the game, they move to a reinforcement learning strategy which does not require reasoning about the beliefs of their opponent. This is consistent with a resource-rational trade-off between the goal of maximising performance and minimizing the cost of computing the best possible actions.

<!--

online experiments results are consistent with behavioural game theory findings, in that human players can deviate from Nash equilibrium play and learn to adapt to the opponent strategy and exploit it when the opponent itself is deviating from Nash equilibrium. Moreover, we showed that participants transfer their learning to new games with varying degrees of similarity. The transfer is also moderated by the level of sophistication of the opponent, with participants showing more success in learning and transferring against opponents adopting a less sophisticated strategy. 

Having said that, there remains a high degree of heterogeneity between players. There is a high positive association between players who learn to beat the sophisticated and less sophisticated opponents, indicating that some players are more able to detect the patterns in opponent play and learn how to exploit them. Moreover, the computational modelling shows that it is likely that players start each game using a model-based learning strategy that facilitates generalisation and opponent model transfer, but then switch to behaviour that is consistent with a  model-free learning strategy as the experiment goes on. This is likely driven by a trade-off between computational complexity and accuracy between model based and model free strategies. 
\newpage
-->

# Declarations 

## Conflict of interest
The authors have no conflicts of interest to declare that are relevant to the content of this article.

## Funding
This work was supported by the UK Engineering and Physical Sciences Research Council under grant EP/S515255/1.  

## Availability of Data and Material
The results are fully reproducible, and all code for the analyses (including the manuscript in R Markdown format) are available at: https://github.com/ismailg/CBB_Learning_Transfer 

## Ethics Approval
The study was conducted in line with the human participant guidelines of the 1964 Declaration of Helsinki and was approved by the local UCL Ethics Committee (EP/2014/005). 

## Consent to Participate and Consent for Publication
Informed consent for participation and publication of results was obtained from all participants at the start of the experiments.

# References

```{r create_r-references, warning= FALSE}
r_refs(file = "Mendeley2.bib")
```